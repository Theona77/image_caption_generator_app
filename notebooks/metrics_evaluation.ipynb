{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdfd079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Tuple\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017d0ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bee4162",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLEUScorer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.smoothing = SmoothingFunction()\n",
    "    \n",
    "    def compute_sentence_bleu(self, reference: List[str], hypothesis: str, n: int = 4) -> float:\n",
    "        # Tokenize\n",
    "        ref_tokens = [ref.split() for ref in reference]\n",
    "        hyp_tokens = hypothesis.split()\n",
    "        \n",
    "        # Set weights based on n\n",
    "        if n == 1:\n",
    "            weights = (1.0, 0, 0, 0)\n",
    "        elif n == 2:\n",
    "            weights = (0.5, 0.5, 0, 0)\n",
    "        elif n == 3:\n",
    "            weights = (0.33, 0.33, 0.33, 0)\n",
    "        else:  # n == 4\n",
    "            weights = (0.25, 0.25, 0.25, 0.25)\n",
    "        \n",
    "        # Compute BLEU\n",
    "        score = sentence_bleu(\n",
    "            ref_tokens, \n",
    "            hyp_tokens, \n",
    "            weights=weights,\n",
    "            smoothing_function=self.smoothing.method1\n",
    "        )\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def compute_corpus_bleu(self, references: List[List[str]], hypotheses: List[str]) -> Dict[str, float]:\n",
    "        # Tokenize all\n",
    "        refs_tokens = [[ref.split() for ref in refs] for refs in references]\n",
    "        hyps_tokens = [hyp.split() for hyp in hypotheses]\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # BLEU-1\n",
    "        results['BLEU-1'] = corpus_bleu(\n",
    "            refs_tokens, hyps_tokens, \n",
    "            weights=(1.0, 0, 0, 0),\n",
    "            smoothing_function=self.smoothing.method1\n",
    "        )\n",
    "        \n",
    "        # BLEU-2\n",
    "        results['BLEU-2'] = corpus_bleu(\n",
    "            refs_tokens, hyps_tokens, \n",
    "            weights=(0.5, 0.5, 0, 0),\n",
    "            smoothing_function=self.smoothing.method1\n",
    "        )\n",
    "        \n",
    "        # BLEU-3\n",
    "        results['BLEU-3'] = corpus_bleu(\n",
    "            refs_tokens, hyps_tokens, \n",
    "            weights=(0.33, 0.33, 0.33, 0),\n",
    "            smoothing_function=self.smoothing.method1\n",
    "        )\n",
    "        \n",
    "        # BLEU-4\n",
    "        results['BLEU-4'] = corpus_bleu(\n",
    "            refs_tokens, hyps_tokens, \n",
    "            weights=(0.25, 0.25, 0.25, 0.25),\n",
    "            smoothing_function=self.smoothing.method1\n",
    "        )\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b84eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class METEORScorer:\n",
    "    \n",
    "    def compute_sentence_meteor(self, reference: List[str], hypothesis: str) -> float:\n",
    "        \"\"\"Compute METEOR for single sentence\"\"\"\n",
    "        # METEOR expects single reference and hypothesis as strings\n",
    "        # We'll average over multiple references\n",
    "        scores = []\n",
    "        for ref in reference:\n",
    "            score = meteor_score([ref.split()], hypothesis.split())\n",
    "            scores.append(score)\n",
    "        \n",
    "        return np.mean(scores)\n",
    "    \n",
    "    def compute_corpus_meteor(self, references: List[List[str]], hypotheses: List[str]) -> float:\n",
    "        \"\"\"Compute METEOR for entire corpus\"\"\"\n",
    "        scores = []\n",
    "        \n",
    "        for refs, hyp in zip(references, hypotheses):\n",
    "            score = self.compute_sentence_meteor(refs, hyp)\n",
    "            scores.append(score)\n",
    "        \n",
    "        return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e9ad5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ROUGEScorer:\n",
    "    \"\"\"\n",
    "    ROUGE-L: Measures longest common subsequence\n",
    "    Focus on recall rather than precision\n",
    "    \n",
    "    Range: 0-1 (higher is better)\n",
    "    Returns: Precision, Recall, F1-score\n",
    "    Good F1 score: > 0.4\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    \n",
    "    def compute_sentence_rouge(self, reference: List[str], hypothesis: str) -> Dict[str, float]:\n",
    "        \"\"\"Compute ROUGE-L for single sentence\"\"\"\n",
    "        scores_list = []\n",
    "        \n",
    "        for ref in reference:\n",
    "            score = self.scorer.score(ref, hypothesis)\n",
    "            scores_list.append({\n",
    "                'precision': score['rougeL'].precision,\n",
    "                'recall': score['rougeL'].recall,\n",
    "                'f1': score['rougeL'].fmeasure\n",
    "            })\n",
    "        \n",
    "        # Average over references\n",
    "        avg_scores = {\n",
    "            'precision': np.mean([s['precision'] for s in scores_list]),\n",
    "            'recall': np.mean([s['recall'] for s in scores_list]),\n",
    "            'f1': np.mean([s['f1'] for s in scores_list])\n",
    "        }\n",
    "        \n",
    "        return avg_scores\n",
    "    \n",
    "    def compute_corpus_rouge(self, references: List[List[str]], hypotheses: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"Compute ROUGE-L for entire corpus\"\"\"\n",
    "        all_precisions = []\n",
    "        all_recalls = []\n",
    "        all_f1s = []\n",
    "        \n",
    "        for refs, hyp in zip(references, hypotheses):\n",
    "            scores = self.compute_sentence_rouge(refs, hyp)\n",
    "            all_precisions.append(scores['precision'])\n",
    "            all_recalls.append(scores['recall'])\n",
    "            all_f1s.append(scores['f1'])\n",
    "        \n",
    "        return {\n",
    "            'ROUGE-L-P': np.mean(all_precisions),\n",
    "            'ROUGE-L-R': np.mean(all_recalls),\n",
    "            'ROUGE-L-F1': np.mean(all_f1s)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eb09ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIDErScorer:\n",
    "    \"\"\"\n",
    "    CIDEr: Specialized metric for image captioning\n",
    "    Measures consensus between generated caption and human captions\n",
    "    Uses TF-IDF weighting\n",
    "    \n",
    "    Range: 0-10+ (higher is better)\n",
    "    Good score: > 0.8\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n=4, sigma=6.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n: max n-gram order\n",
    "            sigma: standard deviation for Gaussian penalty\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    def _compute_doc_freq(self, refs_ngrams):\n",
    "        \"\"\"Compute document frequency for IDF\"\"\"\n",
    "        doc_freq = defaultdict(int)\n",
    "        \n",
    "        for ngrams_dict in refs_ngrams:\n",
    "            for ngram in ngrams_dict.keys():\n",
    "                doc_freq[ngram] += 1\n",
    "        \n",
    "        return doc_freq\n",
    "    \n",
    "    def _get_ngrams(self, tokens, n):\n",
    "        \"\"\"Get n-grams from tokens\"\"\"\n",
    "        ngrams = defaultdict(int)\n",
    "        \n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            ngram = tuple(tokens[i:i+n])\n",
    "            ngrams[ngram] += 1\n",
    "        \n",
    "        return ngrams\n",
    "    \n",
    "    def compute_cider(self, references: List[List[str]], hypotheses: List[str]) -> float:\n",
    "        \"\"\"\n",
    "        Compute CIDEr score\n",
    "        \n",
    "        This is a simplified implementation. For exact CIDEr scores,\n",
    "        use the official pycocoevalcap package.\n",
    "        \"\"\"\n",
    "        scores = []\n",
    "        \n",
    "        # Get all n-grams\n",
    "        all_refs_ngrams = []\n",
    "        all_hyps_ngrams = []\n",
    "        \n",
    "        for refs, hyp in zip(references, hypotheses):\n",
    "            # Tokenize\n",
    "            ref_tokens_list = [ref.split() for ref in refs]\n",
    "            hyp_tokens = hyp.split()\n",
    "            \n",
    "            # Compute n-grams for references\n",
    "            refs_ngrams = []\n",
    "            for ref_tokens in ref_tokens_list:\n",
    "                ngrams_dict = {}\n",
    "                for n in range(1, self.n + 1):\n",
    "                    ngrams_dict.update(self._get_ngrams(ref_tokens, n))\n",
    "                refs_ngrams.append(ngrams_dict)\n",
    "            \n",
    "            # Compute n-grams for hypothesis\n",
    "            hyp_ngrams = {}\n",
    "            for n in range(1, self.n + 1):\n",
    "                hyp_ngrams.update(self._get_ngrams(hyp_tokens, n))\n",
    "            \n",
    "            all_refs_ngrams.append(refs_ngrams)\n",
    "            all_hyps_ngrams.append(hyp_ngrams)\n",
    "        \n",
    "        # Compute document frequencies\n",
    "        all_ngrams = []\n",
    "        for refs_ngrams in all_refs_ngrams:\n",
    "            for ngrams_dict in refs_ngrams:\n",
    "                all_ngrams.append(ngrams_dict)\n",
    "        \n",
    "        doc_freq = self._compute_doc_freq(all_ngrams)\n",
    "        num_docs = len(all_ngrams)\n",
    "        \n",
    "        # Compute CIDEr for each hypothesis\n",
    "        for refs_ngrams, hyp_ngrams in zip(all_refs_ngrams, all_hyps_ngrams):\n",
    "            # Compute TF-IDF vectors\n",
    "            vec_hyp = {}\n",
    "            vec_refs = []\n",
    "            \n",
    "            # Hypothesis vector\n",
    "            for ngram, count in hyp_ngrams.items():\n",
    "                tf = count / len(hyp_ngrams)\n",
    "                idf = np.log((num_docs + 1) / (doc_freq[ngram] + 1))\n",
    "                vec_hyp[ngram] = tf * idf\n",
    "            \n",
    "            # Reference vectors (average)\n",
    "            for ref_ngrams in refs_ngrams:\n",
    "                vec_ref = {}\n",
    "                for ngram, count in ref_ngrams.items():\n",
    "                    tf = count / len(ref_ngrams)\n",
    "                    idf = np.log((num_docs + 1) / (doc_freq[ngram] + 1))\n",
    "                    vec_ref[ngram] = tf * idf\n",
    "                vec_refs.append(vec_ref)\n",
    "            \n",
    "            # Compute cosine similarity\n",
    "            similarities = []\n",
    "            for vec_ref in vec_refs:\n",
    "                # Dot product\n",
    "                dot_product = sum(vec_hyp.get(k, 0) * v for k, v in vec_ref.items())\n",
    "                \n",
    "                # Norms\n",
    "                norm_hyp = np.sqrt(sum(v**2 for v in vec_hyp.values()))\n",
    "                norm_ref = np.sqrt(sum(v**2 for v in vec_ref.values()))\n",
    "                \n",
    "                if norm_hyp > 0 and norm_ref > 0:\n",
    "                    sim = dot_product / (norm_hyp * norm_ref)\n",
    "                else:\n",
    "                    sim = 0.0\n",
    "                \n",
    "                similarities.append(sim)\n",
    "            \n",
    "            # Average similarity\n",
    "            score = np.mean(similarities) * 10.0  # Scale to 0-10\n",
    "            scores.append(score)\n",
    "        \n",
    "        return np.mean(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de457223",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionEvaluator:\n",
    "    \"\"\"\n",
    "    Complete evaluator for image captioning\n",
    "    Computes all major metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.bleu_scorer = BLEUScorer()\n",
    "        self.meteor_scorer = METEORScorer()\n",
    "        self.rouge_scorer = ROUGEScorer()\n",
    "        self.cider_scorer = CIDErScorer()\n",
    "    \n",
    "    def evaluate(self, references: List[List[str]], hypotheses: List[str], \n",
    "                 verbose: bool = True) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate generated captions against references\n",
    "        \n",
    "        Args:\n",
    "            references: List of [list of reference captions] for each image\n",
    "            hypotheses: List of generated captions (one per image)\n",
    "            verbose: Print results\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with all metric scores\n",
    "        \"\"\"\n",
    "        if len(references) != len(hypotheses):\n",
    "            raise ValueError(\"Number of references and hypotheses must match\")\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # 1. BLEU scores\n",
    "        if verbose:\n",
    "            print(\"Computing BLEU scores...\")\n",
    "        bleu_scores = self.bleu_scorer.compute_corpus_bleu(references, hypotheses)\n",
    "        results.update(bleu_scores)\n",
    "        \n",
    "        # 2. METEOR score\n",
    "        if verbose:\n",
    "            print(\"Computing METEOR score...\")\n",
    "        meteor = self.meteor_scorer.compute_corpus_meteor(references, hypotheses)\n",
    "        results['METEOR'] = meteor\n",
    "        \n",
    "        # 3. ROUGE-L score\n",
    "        if verbose:\n",
    "            print(\"Computing ROUGE-L scores...\")\n",
    "        rouge_scores = self.rouge_scorer.compute_corpus_rouge(references, hypotheses)\n",
    "        results.update(rouge_scores)\n",
    "        \n",
    "        # 4. CIDEr score\n",
    "        if verbose:\n",
    "            print(\"Computing CIDEr score...\")\n",
    "        cider = self.cider_scorer.compute_cider(references, hypotheses)\n",
    "        results['CIDEr'] = cider\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"EVALUATION RESULTS\")\n",
    "            print(\"=\"*60)\n",
    "            print(f\"Number of samples: {len(hypotheses)}\")\n",
    "            print()\n",
    "            print(\"BLEU Scores:\")\n",
    "            print(f\"  BLEU-1:  {results['BLEU-1']:.4f}\")\n",
    "            print(f\"  BLEU-2:  {results['BLEU-2']:.4f}\")\n",
    "            print(f\"  BLEU-3:  {results['BLEU-3']:.4f}\")\n",
    "            print(f\"  BLEU-4:  {results['BLEU-4']:.4f}\")\n",
    "            print()\n",
    "            print(f\"METEOR:   {results['METEOR']:.4f}\")\n",
    "            print()\n",
    "            print(\"ROUGE-L Scores:\")\n",
    "            print(f\"  Precision: {results['ROUGE-L-P']:.4f}\")\n",
    "            print(f\"  Recall:    {results['ROUGE-L-R']:.4f}\")\n",
    "            print(f\"  F1-Score:  {results['ROUGE-L-F1']:.4f}\")\n",
    "            print()\n",
    "            print(f\"CIDEr:    {results['CIDEr']:.4f}\")\n",
    "            print(\"=\"*60)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def evaluate_single(self, references: List[str], hypothesis: str) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate single caption\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # BLEU\n",
    "        for n in [1, 2, 3, 4]:\n",
    "            score = self.bleu_scorer.compute_sentence_bleu(references, hypothesis, n)\n",
    "            results[f'BLEU-{n}'] = score\n",
    "        \n",
    "        # METEOR\n",
    "        results['METEOR'] = self.meteor_scorer.compute_sentence_meteor(references, hypothesis)\n",
    "        \n",
    "        # ROUGE-L\n",
    "        rouge = self.rouge_scorer.compute_sentence_rouge(references, hypothesis)\n",
    "        results['ROUGE-L-F1'] = rouge['f1']\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b8dd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_images: List[str], caption_dict: Dict[str, List[str]], \n",
    "                   img_folder: str, tokenizer, config, \n",
    "                   use_beam_search: bool = False) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate model on test set\n",
    "    \n",
    "    Args:\n",
    "        model: Trained captioning model\n",
    "        test_images: List of test image names\n",
    "        caption_dict: Dictionary mapping image names to reference captions\n",
    "        img_folder: Folder containing images\n",
    "        tokenizer: Fitted tokenizer\n",
    "        config: Configuration object\n",
    "        use_beam_search: Use beam search for generation\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with all evaluation metrics\n",
    "    \"\"\"\n",
    "    from image_captioning import generate_caption, generate_beam_search\n",
    "    \n",
    "    print(f\"Evaluating on {len(test_images)} test images...\")\n",
    "    \n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    \n",
    "    for i, img_name in enumerate(test_images):\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f\"Progress: {i+1}/{len(test_images)}\")\n",
    "        \n",
    "        img_path = os.path.join(img_folder, img_name)\n",
    "        \n",
    "        if not os.path.exists(img_path):\n",
    "            continue\n",
    "        \n",
    "        # Get reference captions\n",
    "        refs = caption_dict[img_name]\n",
    "        references.append(refs)\n",
    "        \n",
    "        # Generate caption\n",
    "        try:\n",
    "            if use_beam_search:\n",
    "                hyp = generate_beam_search(model, img_path, tokenizer, config)\n",
    "            else:\n",
    "                hyp = generate_caption(model, img_path, tokenizer, config)\n",
    "            hypotheses.append(hyp)\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating caption for {img_name}: {e}\")\n",
    "            hypotheses.append(\"\")  # Empty caption for failed generation\n",
    "    \n",
    "    # Evaluate\n",
    "    evaluator = CaptionEvaluator()\n",
    "    results = evaluator.evaluate(references, hypotheses, verbose=True)\n",
    "    \n",
    "    return results, references, hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a54d3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_predictions(references: List[List[str]], hypotheses: List[str], \n",
    "                        image_names: List[str] = None, n_samples: int = 10):\n",
    "    \"\"\"\n",
    "    Show qualitative examples of predictions\n",
    "    \n",
    "    Args:\n",
    "        references: Reference captions\n",
    "        hypotheses: Generated captions\n",
    "        image_names: Optional image names\n",
    "        n_samples: Number of samples to show\n",
    "    \"\"\"\n",
    "    evaluator = CaptionEvaluator()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"QUALITATIVE ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Select random samples\n",
    "    indices = np.random.choice(len(hypotheses), min(n_samples, len(hypotheses)), replace=False)\n",
    "    \n",
    "    for idx in indices:\n",
    "        refs = references[idx]\n",
    "        hyp = hypotheses[idx]\n",
    "        \n",
    "        # Compute metrics for this sample\n",
    "        scores = evaluator.evaluate_single(refs, hyp)\n",
    "        \n",
    "        print(f\"\\nSample {idx+1}\")\n",
    "        if image_names:\n",
    "            print(f\"Image: {image_names[idx]}\")\n",
    "        print(\"-\" * 80)\n",
    "        print(\"Reference Captions:\")\n",
    "        for i, ref in enumerate(refs, 1):\n",
    "            print(f\"  {i}. {ref}\")\n",
    "        print(f\"\\nGenerated Caption:\")\n",
    "        print(f\"  → {hyp}\")\n",
    "        print(f\"\\nScores:\")\n",
    "        print(f\"  BLEU-4: {scores['BLEU-4']:.4f}\")\n",
    "        print(f\"  METEOR: {scores['METEOR']:.4f}\")\n",
    "        print(f\"  ROUGE-L: {scores['ROUGE-L-F1']:.4f}\")\n",
    "        print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827d9293",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_evaluation_results(results: Dict[str, float], references: List[List[str]], \n",
    "                            hypotheses: List[str], image_names: List[str] = None,\n",
    "                            output_dir: str = \"evaluation_results\"):\n",
    "    \"\"\"Save evaluation results to files\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # 1. Save metrics\n",
    "    with open(os.path.join(output_dir, \"metrics.json\"), 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    # 2. Save predictions\n",
    "    predictions_data = []\n",
    "    for i, (refs, hyp) in enumerate(zip(references, hypotheses)):\n",
    "        entry = {\n",
    "            'id': i,\n",
    "            'references': refs,\n",
    "            'hypothesis': hyp\n",
    "        }\n",
    "        if image_names:\n",
    "            entry['image'] = image_names[i]\n",
    "        predictions_data.append(entry)\n",
    "    \n",
    "    with open(os.path.join(output_dir, \"predictions.json\"), 'w') as f:\n",
    "        json.dump(predictions_data, f, indent=2)\n",
    "    \n",
    "    # 3. Save as CSV for easy viewing\n",
    "    df = pd.DataFrame({\n",
    "        'image': image_names if image_names else range(len(hypotheses)),\n",
    "        'generated': hypotheses,\n",
    "        'reference_1': [refs[0] if len(refs) > 0 else \"\" for refs in references],\n",
    "        'reference_2': [refs[1] if len(refs) > 1 else \"\" for refs in references],\n",
    "    })\n",
    "    df.to_csv(os.path.join(output_dir, \"predictions.csv\"), index=False)\n",
    "    \n",
    "    print(f\"\\nResults saved to {output_dir}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d5448c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_usage():\n",
    "    \"\"\"Example of how to use the evaluation functions\"\"\"\n",
    "    \n",
    "    # Example data\n",
    "    references = [\n",
    "        [\"a dog playing in the park\", \"a brown dog running on grass\"],\n",
    "        [\"a cat sitting on a chair\", \"an orange cat on furniture\"],\n",
    "        [\"a car on the street\", \"a red vehicle on the road\"]\n",
    "    ]\n",
    "    \n",
    "    hypotheses = [\n",
    "        \"a dog running in a park\",\n",
    "        \"a cat sitting on a chair\",\n",
    "        \"a red car on the street\"\n",
    "    ]\n",
    "    \n",
    "    # Evaluate\n",
    "    evaluator = CaptionEvaluator()\n",
    "    results = evaluator.evaluate(references, hypotheses, verbose=True)\n",
    "    \n",
    "    # Show qualitative analysis\n",
    "    analyze_predictions(references, hypotheses, n_samples=3)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cf4558",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_evaluation(model_path: str, test_images: List[str], \n",
    "                    caption_dict: Dict[str, List[str]], img_folder: str,\n",
    "                    tokenizer_path: str, config, use_beam_search: bool = True):\n",
    "    \"\"\"\n",
    "    Complete evaluation pipeline\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to saved model\n",
    "        test_images: List of test image filenames\n",
    "        caption_dict: Dictionary of captions\n",
    "        img_folder: Folder containing images\n",
    "        tokenizer_path: Path to saved tokenizer\n",
    "        config: Configuration object\n",
    "        use_beam_search: Use beam search for generation\n",
    "    \"\"\"\n",
    "    import tensorflow as tf\n",
    "    import pickle\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    print(\"Loading model and tokenizer...\")\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    \n",
    "    with open(tokenizer_path, 'rb') as f:\n",
    "        tokenizer = pickle.load(f)\n",
    "    \n",
    "    # Evaluate\n",
    "    results, references, hypotheses = evaluate_model(\n",
    "        model, test_images, caption_dict, img_folder, \n",
    "        tokenizer, config, use_beam_search\n",
    "    )\n",
    "    \n",
    "    # Qualitative analysis\n",
    "    analyze_predictions(references, hypotheses, test_images, n_samples=10)\n",
    "    \n",
    "    # Save results\n",
    "    save_evaluation_results(results, references, hypotheses, test_images)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e3aab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing evaluation metrics...\n",
      "\n",
      "Computing BLEU scores...\n",
      "Computing METEOR score...\n",
      "Computing ROUGE-L scores...\n",
      "Computing CIDEr score...\n",
      "\n",
      "============================================================\n",
      "EVALUATION RESULTS\n",
      "============================================================\n",
      "Number of samples: 3\n",
      "\n",
      "BLEU Scores:\n",
      "  BLEU-1:  0.9444\n",
      "  BLEU-2:  0.8322\n",
      "  BLEU-3:  0.7047\n",
      "  BLEU-4:  0.6263\n",
      "\n",
      "METEOR:   0.6562\n",
      "\n",
      "ROUGE-L Scores:\n",
      "  Precision: 0.6667\n",
      "  Recall:    0.7056\n",
      "  F1-Score:  0.6843\n",
      "\n",
      "CIDEr:    3.2793\n",
      "============================================================\n",
      "\n",
      "================================================================================\n",
      "QUALITATIVE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Sample 1\n",
      "--------------------------------------------------------------------------------\n",
      "Reference Captions:\n",
      "  1. a dog playing in the park\n",
      "  2. a brown dog running on grass\n",
      "\n",
      "Generated Caption:\n",
      "  → a dog running in a park\n",
      "\n",
      "Scores:\n",
      "  BLEU-4: 0.1291\n",
      "  METEOR: 0.5846\n",
      "  ROUGE-L: 0.5833\n",
      "================================================================================\n",
      "\n",
      "Sample 3\n",
      "--------------------------------------------------------------------------------\n",
      "Reference Captions:\n",
      "  1. a car on the street\n",
      "  2. a red vehicle on the road\n",
      "\n",
      "Generated Caption:\n",
      "  → a red car on the street\n",
      "\n",
      "Scores:\n",
      "  BLEU-4: 0.6043\n",
      "  METEOR: 0.7870\n",
      "  ROUGE-L: 0.7879\n",
      "================================================================================\n",
      "\n",
      "Sample 2\n",
      "--------------------------------------------------------------------------------\n",
      "Reference Captions:\n",
      "  1. a cat sitting on a chair\n",
      "  2. an orange cat on furniture\n",
      "\n",
      "Generated Caption:\n",
      "  → a cat sitting on a chair\n",
      "\n",
      "Scores:\n",
      "  BLEU-4: 1.0000\n",
      "  METEOR: 0.5969\n",
      "  ROUGE-L: 0.6818\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Simple test\n",
    "    print(\"Testing evaluation metrics...\\n\")\n",
    "    example_usage()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
