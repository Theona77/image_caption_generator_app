{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "649b1207",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #### Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a756e8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "\n",
    "# #### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b80839b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Config:\n",
    "    # Paths\n",
    "    CAPTION_FILE = \"captions.txt\"  \n",
    "    IMAGE_FOLDER = \"Images\"  \n",
    "    SAVE_DIR = \"saved_models\"\n",
    "    \n",
    "    # Image settings\n",
    "    IMAGE_SIZE = (299, 299)  \n",
    "    \n",
    "    # Model hyperparameters\n",
    "    EMBED_DIM = 256\n",
    "    NUM_HEADS = 4\n",
    "    FF_DIM = 512\n",
    "    NUM_LAYERS = 2\n",
    "    \n",
    "    # Training settings\n",
    "    MAX_LEN = 40  \n",
    "    VOCAB_SIZE = 10000\n",
    "    BATCH_SIZE = 16\n",
    "    EPOCHS = 20\n",
    "    LEARNING_RATE = 1e-4\n",
    "    \n",
    "    # Data split\n",
    "    TRAIN_SPLIT = 0.8\n",
    "    VAL_SPLIT = 0.1\n",
    "    TEST_SPLIT = 0.1\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(config.SAVE_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# #### Data loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43ed609f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def clean_caption(text):\n",
    "    \"\"\"Clean and normalize caption text\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)  # Remove special chars\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    return text\n",
    "\n",
    "def load_captions(caption_file):\n",
    "    \"\"\"Load and parse captions file\"\"\"\n",
    "    print(\"Loading captions...\")\n",
    "    df = pd.read_csv(caption_file, names=[\"image\", \"caption\"])\n",
    "    df['caption'] = df['caption'].astype(str).apply(clean_caption)\n",
    "    df = df[df['caption'].str.len() > 0]  # Remove empty captions\n",
    "    \n",
    "    # Group captions by image\n",
    "    caption_dict = df.groupby(\"image\")[\"caption\"].apply(list).to_dict()\n",
    "    print(f\"Loaded {len(caption_dict)} images with {len(df)} captions\")\n",
    "    \n",
    "    return caption_dict\n",
    "\n",
    "\n",
    "# #### Tokenizer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "219cb4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_tokenizer(caption_dict, vocab_size):\n",
    "    \"\"\"Create tokenizer with special tokens\"\"\"\n",
    "    print(\"Creating tokenizer...\")\n",
    "    \n",
    "    # Prepare all captions with special tokens\n",
    "    all_captions = []\n",
    "    for captions in caption_dict.values():\n",
    "        for cap in captions:\n",
    "            text = f\"<start> {cap} <end>\"\n",
    "            all_captions.append(text)\n",
    "    \n",
    "    # Fit tokenizer\n",
    "    tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<unk>\", filters='')\n",
    "    tokenizer.fit_on_texts(all_captions)\n",
    "    \n",
    "    vocab_size = min(vocab_size, len(tokenizer.word_index) + 1)\n",
    "    print(f\"Vocabulary size: {vocab_size}\")\n",
    "    \n",
    "    return tokenizer, vocab_size\n",
    "\n",
    "\n",
    "# #### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9074aa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_image(img_path, img_size):\n",
    "    \"\"\"Load and preprocess image\"\"\"\n",
    "    img = Image.open(img_path).convert(\"RGB\").resize(img_size)\n",
    "    img_array = np.array(img).astype(\"float32\") / 255.0\n",
    "    return img_array\n",
    "\n",
    "def caption_to_sequences(caption, tokenizer, max_len):\n",
    "    \"\"\"Convert caption to input and output sequences\"\"\"\n",
    "    text = f\"<start> {caption} <end>\"\n",
    "    seq = tokenizer.texts_to_sequences([text])[0]\n",
    "    \n",
    "    # Input: without last token, Output: without first token\n",
    "    seq_in = seq[:-1]\n",
    "    seq_out = seq[1:]\n",
    "    \n",
    "    # Pad sequences\n",
    "    seq_in = pad_sequences([seq_in], maxlen=max_len, padding='post')[0]\n",
    "    seq_out = pad_sequences([seq_out], maxlen=max_len, padding='post')[0]\n",
    "    \n",
    "    return seq_in, seq_out\n",
    "\n",
    "\n",
    "# #### Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b1f98997",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    \"\"\"Efficient data generator for training\"\"\"\n",
    "    \n",
    "    def __init__(self, image_list, caption_dict, img_folder, tokenizer, \n",
    "                 batch_size, max_len, img_size, shuffle=True):\n",
    "        self.image_list = image_list\n",
    "        self.caption_dict = caption_dict\n",
    "        self.img_folder = img_folder\n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_size = batch_size\n",
    "        self.max_len = max_len\n",
    "        self.img_size = img_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.image_list) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_images = [self.image_list[k] for k in indices]\n",
    "        return self._generate_batch(batch_images)\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indices = np.arange(len(self.image_list))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "    \n",
    "    def _generate_batch(self, batch_images):\n",
    "        batch_imgs = []\n",
    "        batch_cap_in = []\n",
    "        batch_cap_out = []\n",
    "        \n",
    "        for img_name in batch_images:\n",
    "            img_path = os.path.join(self.img_folder, img_name)\n",
    "            \n",
    "            if not os.path.exists(img_path):\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Load image\n",
    "                img = load_image(img_path, self.img_size)\n",
    "                \n",
    "                # Random caption for variety\n",
    "                caption = random.choice(self.caption_dict[img_name])\n",
    "                seq_in, seq_out = caption_to_sequences(caption, self.tokenizer, self.max_len)\n",
    "                \n",
    "                batch_imgs.append(img)\n",
    "                batch_cap_in.append(seq_in)\n",
    "                batch_cap_out.append(seq_out)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {img_name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if len(batch_imgs) == 0:\n",
    "            # Return empty batch (shouldn't happen often)\n",
    "            return [np.zeros((1, *self.img_size, 3)), \n",
    "                    np.zeros((1, self.max_len))], np.zeros((1, self.max_len))\n",
    "        \n",
    "        return (np.array(batch_imgs), np.array(batch_cap_in)), np.array(batch_cap_out)\n",
    "\n",
    "\n",
    "\n",
    "# #### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c208d0fb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    \"\"\"Create causal mask for decoder self-attention\"\"\"\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "75166fef",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def build_cnn_encoder(img_size, embed_dim):\n",
    "    \"\"\"CNN Encoder using EfficientNetB0 with spatial features\"\"\"\n",
    "    print(\"Building CNN Encoder...\")\n",
    "    \n",
    "    base_model = tf.keras.applications.EfficientNetB0(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=(*img_size, 3),\n",
    "        pooling=None  # Keep spatial features\n",
    "    )\n",
    "    base_model.trainable = False  # Freeze for faster training\n",
    "    \n",
    "    inp = layers.Input(shape=(*img_size, 3), name='image_input')\n",
    "    x = base_model(inp)  # (batch, H, W, C)\n",
    "    \n",
    "    # Reshape to sequence\n",
    "    h, w, c = x.shape[1], x.shape[2], x.shape[3]\n",
    "    x = layers.Reshape((h*w, c))(x)\n",
    "\n",
    "    x = layers.Dense(embed_dim, activation='relu')(x)\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    \n",
    "    model = models.Model(inp, x, name='cnn_encoder')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e6a7a0e6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def transformer_decoder_block(x, img_features, embed_dim, num_heads, ff_dim, \n",
    "                               causal_mask, dropout=0.1):\n",
    "    \"\"\"Single Transformer Decoder Block\"\"\"\n",
    "    \n",
    "    # 1. Self-attention with causal mask\n",
    "    attn1 = layers.MultiHeadAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=embed_dim,\n",
    "        dropout=dropout\n",
    "    )(query=x, value=x, key=x, attention_mask=causal_mask)\n",
    "    x = layers.LayerNormalization()(x + attn1)\n",
    "    \n",
    "    # 2. Cross-attention to image features\n",
    "    attn2 = layers.MultiHeadAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=embed_dim,\n",
    "        dropout=dropout\n",
    "    )(query=x, value=img_features, key=img_features)\n",
    "    x = layers.LayerNormalization()(x + attn2)\n",
    "    \n",
    "    # 3. Feed-forward network\n",
    "    ffn = layers.Dense(ff_dim, activation='relu')(x)\n",
    "    ffn = layers.Dropout(dropout)(ffn)\n",
    "    ffn = layers.Dense(embed_dim)(ffn)\n",
    "    x = layers.LayerNormalization()(x + ffn)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dc436481",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def build_transformer_decoder(vocab_size, embed_dim, num_heads, ff_dim, \n",
    "                               num_layers, max_len):\n",
    "    \"\"\"Transformer Decoder with causal masking\"\"\"\n",
    "    print(\"Building Transformer Decoder...\")\n",
    "    \n",
    "    # Inputs\n",
    "    img_features = layers.Input(shape=(None, embed_dim), name='img_features')\n",
    "    caption_input = layers.Input(shape=(max_len,), dtype='int32', name='caption_input')\n",
    "    \n",
    "    # Token embeddings\n",
    "    token_emb = layers.Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embed_dim,\n",
    "        mask_zero=True,\n",
    "        name='token_embedding'\n",
    "    )(caption_input)\n",
    "    \n",
    "    # Positional embeddings\n",
    "    positions = tf.range(start=0, limit=max_len, delta=1)\n",
    "    pos_emb = layers.Embedding(\n",
    "        input_dim=max_len,\n",
    "        output_dim=embed_dim,\n",
    "        name='position_embedding'\n",
    "    )(positions)\n",
    "    \n",
    "    x = token_emb + pos_emb\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    \n",
    "    # Create causal mask\n",
    "    causal_mask = create_look_ahead_mask(max_len)\n",
    "    \n",
    "    # Stack transformer blocks\n",
    "    for i in range(num_layers):\n",
    "        x = transformer_decoder_block(\n",
    "            x, img_features, embed_dim, num_heads, ff_dim, \n",
    "            causal_mask, dropout=0.1\n",
    "        )\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = layers.Dense(vocab_size, activation='softmax', name='output')(x)\n",
    "    \n",
    "    model = models.Model([img_features, caption_input], outputs, name='transformer_decoder')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0d9f0028",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def build_complete_model(config, vocab_size):\n",
    "    \"\"\"Build complete end-to-end model\"\"\"\n",
    "    print(\"\\nBuilding complete model...\")\n",
    "    \n",
    "    # Encoder\n",
    "    encoder = build_cnn_encoder(config.IMAGE_SIZE, config.EMBED_DIM)\n",
    "    \n",
    "    # Decoder\n",
    "    decoder = build_transformer_decoder(\n",
    "        vocab_size=vocab_size,\n",
    "        embed_dim=config.EMBED_DIM,\n",
    "        num_heads=config.NUM_HEADS,\n",
    "        ff_dim=config.FF_DIM,\n",
    "        num_layers=config.NUM_LAYERS,\n",
    "        max_len=config.MAX_LEN\n",
    "    )\n",
    "    \n",
    "    # Full model\n",
    "    img_input = layers.Input(shape=(*config.IMAGE_SIZE, 3), name='image')\n",
    "    cap_input = layers.Input(shape=(config.MAX_LEN,), dtype='int32', name='caption')\n",
    "    \n",
    "    img_features = encoder(img_input)\n",
    "    outputs = decoder([img_features, cap_input])\n",
    "    \n",
    "    model = models.Model([img_input, cap_input], outputs, name='image_captioning')\n",
    "    \n",
    "    return model, encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f867f1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_model(model, train_gen, val_gen, config):\n",
    "    \"\"\"Train the model with callbacks\"\"\"\n",
    "    print(\"\\nCompiling model...\")\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=config.LEARNING_RATE),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            filepath=os.path.join(config.SAVE_DIR, 'best_model.h5'),\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=3,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        ),\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nStarting training...\")\n",
    "    history = model.fit(\n",
    "        train_gen,\n",
    "        validation_data=val_gen,\n",
    "        epochs=config.EPOCHS,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "# #### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "40df703b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_caption(model, image_path, tokenizer, config, max_len=None):\n",
    "    \"\"\"Generate caption for a single image using greedy decoding\"\"\"\n",
    "    if max_len is None:\n",
    "        max_len = config.MAX_LEN\n",
    "    \n",
    "    # Load and preprocess image\n",
    "    img = load_image(image_path, config.IMAGE_SIZE)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    \n",
    "    # Get special tokens\n",
    "    start_token = tokenizer.word_index.get('<start>', 1)\n",
    "    end_token = tokenizer.word_index.get('<end>', 2)\n",
    "    \n",
    "    # Initialize caption with start token\n",
    "    caption_seq = [start_token]\n",
    "    \n",
    "    # Generate caption word by word\n",
    "    for _ in range(max_len):\n",
    "        # Pad current sequence\n",
    "        padded_seq = pad_sequences([caption_seq], maxlen=max_len, padding='post')\n",
    "        \n",
    "        # Predict next token\n",
    "        predictions = model.predict([img, padded_seq], verbose=0)\n",
    "        \n",
    "        # Get prediction at current position\n",
    "        current_pos = len(caption_seq) - 1\n",
    "        if current_pos >= max_len:\n",
    "            break\n",
    "        \n",
    "        predicted_id = np.argmax(predictions[0, current_pos])\n",
    "        \n",
    "        # Stop if end token\n",
    "        if predicted_id == end_token:\n",
    "            break\n",
    "        \n",
    "        caption_seq.append(predicted_id)\n",
    "    \n",
    "    # Convert tokens to words\n",
    "    inv_vocab = {v: k for k, v in tokenizer.word_index.items()}\n",
    "    caption_words = []\n",
    "    \n",
    "    for token_id in caption_seq:\n",
    "        if token_id == 0:  # padding\n",
    "            continue\n",
    "        word = inv_vocab.get(token_id, '<unk>')\n",
    "        if word in ['<start>', '<end>']:\n",
    "            continue\n",
    "        caption_words.append(word)\n",
    "    \n",
    "    return ' '.join(caption_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "31da3499",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_beam_search(model, image_path, tokenizer, config, beam_width=3):\n",
    "    \"\"\"Generate caption using beam search (better quality)\"\"\"\n",
    "    img = load_image(image_path, config.IMAGE_SIZE)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    \n",
    "    start_token = tokenizer.word_index.get('<start>', 1)\n",
    "    end_token = tokenizer.word_index.get('<end>', 2)\n",
    "    \n",
    "    # Initialize beam with start token\n",
    "    beams = [([start_token], 0.0)]  # (sequence, score)\n",
    "    \n",
    "    for _ in range(config.MAX_LEN):\n",
    "        new_beams = []\n",
    "        \n",
    "        for seq, score in beams:\n",
    "            if seq[-1] == end_token:\n",
    "                new_beams.append((seq, score))\n",
    "                continue\n",
    "            \n",
    "            # Pad and predict\n",
    "            padded = pad_sequences([seq], maxlen=config.MAX_LEN, padding='post')\n",
    "            preds = model.predict([img, padded], verbose=0)[0, len(seq)-1]\n",
    "            \n",
    "            # Get top k predictions\n",
    "            top_k = np.argsort(preds)[-beam_width:]\n",
    "            \n",
    "            for token_id in top_k:\n",
    "                new_seq = seq + [token_id]\n",
    "                new_score = score - np.log(preds[token_id] + 1e-10)\n",
    "                new_beams.append((new_seq, new_score))\n",
    "        \n",
    "        # Keep top beam_width beams\n",
    "        beams = sorted(new_beams, key=lambda x: x[1])[:beam_width]\n",
    "    \n",
    "    # Get best sequence\n",
    "    best_seq = beams[0][0]\n",
    "    \n",
    "    # Convert to words\n",
    "    inv_vocab = {v: k for k, v in tokenizer.word_index.items()}\n",
    "    words = [inv_vocab.get(tid, '<unk>') for tid in best_seq \n",
    "             if tid not in [0, start_token, end_token]]\n",
    "    \n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8bdc2bdf",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training pipeline\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"IMAGE CAPTIONING: CNN + TRANSFORMER\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # 1. Load data\n",
    "    caption_dict = load_captions(config.CAPTION_FILE)\n",
    "    \n",
    "    # 2. Create tokenizer\n",
    "    tokenizer, vocab_size = create_tokenizer(caption_dict, config.VOCAB_SIZE)\n",
    "    \n",
    "    # Save tokenizer\n",
    "    import pickle\n",
    "    with open(os.path.join(config.SAVE_DIR, 'tokenizer.pkl'), 'wb') as f:\n",
    "        pickle.dump(tokenizer, f)\n",
    "    \n",
    "    # 3. Split data\n",
    "    all_images = list(caption_dict.keys())\n",
    "    random.shuffle(all_images)\n",
    "    \n",
    "    train_size = int(len(all_images) * config.TRAIN_SPLIT)\n",
    "    val_size = int(len(all_images) * config.VAL_SPLIT)\n",
    "    \n",
    "    train_images = all_images[:train_size]\n",
    "    val_images = all_images[train_size:train_size + val_size]\n",
    "    test_images = all_images[train_size + val_size:]\n",
    "    \n",
    "    print(f\"\\nDataset split:\")\n",
    "    print(f\"  Train: {len(train_images)}\")\n",
    "    print(f\"  Val:   {len(val_images)}\")\n",
    "    print(f\"  Test:  {len(test_images)}\")\n",
    "    \n",
    "    # 4. Create generators\n",
    "    train_gen = DataGenerator(\n",
    "        train_images, caption_dict, config.IMAGE_FOLDER,\n",
    "        tokenizer, config.BATCH_SIZE, config.MAX_LEN, config.IMAGE_SIZE\n",
    "    )\n",
    "    \n",
    "    val_gen = DataGenerator(\n",
    "        val_images, caption_dict, config.IMAGE_FOLDER,\n",
    "        tokenizer, config.BATCH_SIZE, config.MAX_LEN, config.IMAGE_SIZE,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # 5. Build model\n",
    "    model, encoder, decoder = build_complete_model(config, vocab_size)\n",
    "    model.summary()\n",
    "    \n",
    "    # 6. Train\n",
    "    history = train_model(model, train_gen, val_gen, config)\n",
    "    \n",
    "    # 7. Save final model\n",
    "    model.save(os.path.join(config.SAVE_DIR, 'final_model.h5'))\n",
    "    print(f\"\\nModel saved to {config.SAVE_DIR}\")\n",
    "    \n",
    "    # 8. Test inference\n",
    "    if len(test_images) > 0:\n",
    "        test_img = os.path.join(config.IMAGE_FOLDER, test_images[0])\n",
    "        print(f\"\\nTest inference on: {test_images[0]}\")\n",
    "        caption = generate_caption(model, test_img, tokenizer, config)\n",
    "        print(f\"Generated caption: {caption}\")\n",
    "    \n",
    "    return model, tokenizer, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5673991d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_and_predict(image_path, use_beam_search=False):\n",
    "    \"\"\"Load saved model and generate caption\"\"\"\n",
    "    import pickle\n",
    "    \n",
    "    # Load tokenizer\n",
    "    with open(os.path.join(config.SAVE_DIR, 'tokenizer.pkl'), 'rb') as f:\n",
    "        tokenizer = pickle.load(f)\n",
    "    \n",
    "    # Load model\n",
    "    model = tf.keras.models.load_model(os.path.join(config.SAVE_DIR, 'best_model.h5'))\n",
    "    \n",
    "    # Generate caption\n",
    "    if use_beam_search:\n",
    "        caption = generate_beam_search(model, image_path, tokenizer, config)\n",
    "    else:\n",
    "        caption = generate_caption(model, image_path, tokenizer, config)\n",
    "    \n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b711e37f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "IMAGE CAPTIONING: CNN + TRANSFORMER\n",
      "======================================================================\n",
      "Loading captions...\n",
      "Loaded 8092 images with 40456 captions\n",
      "Creating tokenizer...\n",
      "Vocabulary size: 8833\n",
      "\n",
      "Dataset split:\n",
      "  Train: 6473\n",
      "  Val:   809\n",
      "  Test:  810\n",
      "\n",
      "Building complete model...\n",
      "Building CNN Encoder...\n",
      "Building Transformer Decoder...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"image_captioning\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"image_captioning\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ image (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">299</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">299</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ cnn_encoder         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">4,378,019</span> │ image[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ caption             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_decoder │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8833</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">9,267,841</span> │ cnn_encoder[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │ caption[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ image (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m299\u001b[0m, \u001b[38;5;34m299\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│                     │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ cnn_encoder         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │  \u001b[38;5;34m4,378,019\u001b[0m │ image[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ caption             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_decoder │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m8833\u001b[0m)  │  \u001b[38;5;34m9,267,841\u001b[0m │ cnn_encoder[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │ caption[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,645,860</span> (52.05 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m13,645,860\u001b[0m (52.05 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,596,289</span> (36.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m9,596,289\u001b[0m (36.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> (15.45 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m4,049,571\u001b[0m (15.45 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compiling model...\n",
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 885ms/step - accuracy: 0.6982 - loss: 4.0884\n",
      "Epoch 1: val_loss improved from None to 1.43080, saving model to saved_models\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m417s\u001b[0m 978ms/step - accuracy: 0.7432 - loss: 2.5845 - val_accuracy: 0.7902 - val_loss: 1.4308 - learning_rate: 1.0000e-04\n",
      "Epoch 2/20\n",
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 903ms/step - accuracy: 0.8095 - loss: 1.2860\n",
      "Epoch 2: val_loss improved from 1.43080 to 0.96617, saving model to saved_models\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m397s\u001b[0m 982ms/step - accuracy: 0.8264 - loss: 1.1726 - val_accuracy: 0.8565 - val_loss: 0.9662 - learning_rate: 1.0000e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 872ms/step - accuracy: 0.8715 - loss: 0.8712\n",
      "Epoch 3: val_loss improved from 0.96617 to 0.66254, saving model to saved_models\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m381s\u001b[0m 944ms/step - accuracy: 0.8847 - loss: 0.7989 - val_accuracy: 0.9088 - val_loss: 0.6625 - learning_rate: 1.0000e-04\n",
      "Epoch 4/20\n",
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 792ms/step - accuracy: 0.9170 - loss: 0.6099\n",
      "Epoch 4: val_loss improved from 0.66254 to 0.48366, saving model to saved_models\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m350s\u001b[0m 867ms/step - accuracy: 0.9245 - loss: 0.5645 - val_accuracy: 0.9368 - val_loss: 0.4837 - learning_rate: 1.0000e-04\n",
      "Epoch 5/20\n",
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9421 - loss: 0.4472\n",
      "Epoch 5: val_loss improved from 0.48366 to 0.37076, saving model to saved_models\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1270s\u001b[0m 3s/step - accuracy: 0.9460 - loss: 0.4223 - val_accuracy: 0.9544 - val_loss: 0.3708 - learning_rate: 1.0000e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 833ms/step - accuracy: 0.9555 - loss: 0.3542\n",
      "Epoch 6: val_loss improved from 0.37076 to 0.29082, saving model to saved_models\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can't decrement id ref count (unable to extend file properly)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m# For training:\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     model, tokenizer, history = \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 51\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     48\u001b[39m model.summary()\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# 6. Train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m history = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# 7. Save final model\u001b[39;00m\n\u001b[32m     54\u001b[39m model.save(os.path.join(config.SAVE_DIR, \u001b[33m'\u001b[39m\u001b[33mfinal_model.h5\u001b[39m\u001b[33m'\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_gen, val_gen, config)\u001b[39m\n\u001b[32m     12\u001b[39m callbacks = [\n\u001b[32m     13\u001b[39m     ModelCheckpoint(\n\u001b[32m     14\u001b[39m         filepath=os.path.join(config.SAVE_DIR, \u001b[33m'\u001b[39m\u001b[33mbest_model.h5\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m     31\u001b[39m     )\n\u001b[32m     32\u001b[39m ]\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mStarting training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_gen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_gen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m     41\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m history\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\h5py\\_hl\\files.py:632\u001b[39m, in \u001b[36mFile.close\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    626\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.id.valid:\n\u001b[32m    627\u001b[39m     \u001b[38;5;66;03m# We have to explicitly murder all open objects related to the file\u001b[39;00m\n\u001b[32m    628\u001b[39m \n\u001b[32m    629\u001b[39m     \u001b[38;5;66;03m# Close file-resident objects first, then the files.\u001b[39;00m\n\u001b[32m    630\u001b[39m     \u001b[38;5;66;03m# Otherwise we get errors in MPI mode.\u001b[39;00m\n\u001b[32m    631\u001b[39m     \u001b[38;5;28mself\u001b[39m.id._close_open_objects(h5f.OBJ_LOCAL | ~h5f.OBJ_FILE)\n\u001b[32m--> \u001b[39m\u001b[32m632\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mid\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_close_open_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh5f\u001b[49m\u001b[43m.\u001b[49m\u001b[43mOBJ_LOCAL\u001b[49m\u001b[43m \u001b[49m\u001b[43m|\u001b[49m\u001b[43m \u001b[49m\u001b[43mh5f\u001b[49m\u001b[43m.\u001b[49m\u001b[43mOBJ_FILE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    634\u001b[39m     \u001b[38;5;28mself\u001b[39m.id.close()\n\u001b[32m    635\u001b[39m     _objects.nonlocal_close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:54\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:55\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/h5f.pyx:357\u001b[39m, in \u001b[36mh5py.h5f.FileID._close_open_objects\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mRuntimeError\u001b[39m: Can't decrement id ref count (unable to extend file properly)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # For training:\n",
    "    model, tokenizer, history = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2396e4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Tuple\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2768354",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98adac69",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class BLEUScorer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.smoothing = SmoothingFunction()\n",
    "    \n",
    "    def compute_sentence_bleu(self, reference: List[str], hypothesis: str, n: int = 4) -> float:\n",
    "        # Tokenize\n",
    "        ref_tokens = [ref.split() for ref in reference]\n",
    "        hyp_tokens = hypothesis.split()\n",
    "        \n",
    "        # Set weights based on n\n",
    "        if n == 1:\n",
    "            weights = (1.0, 0, 0, 0)\n",
    "        elif n == 2:\n",
    "            weights = (0.5, 0.5, 0, 0)\n",
    "        elif n == 3:\n",
    "            weights = (0.33, 0.33, 0.33, 0)\n",
    "        else:  # n == 4\n",
    "            weights = (0.25, 0.25, 0.25, 0.25)\n",
    "        \n",
    "        # Compute BLEU\n",
    "        score = sentence_bleu(\n",
    "            ref_tokens, \n",
    "            hyp_tokens, \n",
    "            weights=weights,\n",
    "            smoothing_function=self.smoothing.method1\n",
    "        )\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def compute_corpus_bleu(self, references: List[List[str]], hypotheses: List[str]) -> Dict[str, float]:\n",
    "        # Tokenize all\n",
    "        refs_tokens = [[ref.split() for ref in refs] for refs in references]\n",
    "        hyps_tokens = [hyp.split() for hyp in hypotheses]\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # BLEU-1\n",
    "        results['BLEU-1'] = corpus_bleu(\n",
    "            refs_tokens, hyps_tokens, \n",
    "            weights=(1.0, 0, 0, 0),\n",
    "            smoothing_function=self.smoothing.method1\n",
    "        )\n",
    "        \n",
    "        # BLEU-2\n",
    "        results['BLEU-2'] = corpus_bleu(\n",
    "            refs_tokens, hyps_tokens, \n",
    "            weights=(0.5, 0.5, 0, 0),\n",
    "            smoothing_function=self.smoothing.method1\n",
    "        )\n",
    "        \n",
    "        # BLEU-3\n",
    "        results['BLEU-3'] = corpus_bleu(\n",
    "            refs_tokens, hyps_tokens, \n",
    "            weights=(0.33, 0.33, 0.33, 0),\n",
    "            smoothing_function=self.smoothing.method1\n",
    "        )\n",
    "        \n",
    "        # BLEU-4\n",
    "        results['BLEU-4'] = corpus_bleu(\n",
    "            refs_tokens, hyps_tokens, \n",
    "            weights=(0.25, 0.25, 0.25, 0.25),\n",
    "            smoothing_function=self.smoothing.method1\n",
    "        )\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cfda98",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class METEORScorer:\n",
    "    \n",
    "    def compute_sentence_meteor(self, reference: List[str], hypothesis: str) -> float:\n",
    "        \"\"\"Compute METEOR for single sentence\"\"\"\n",
    "        # METEOR expects single reference and hypothesis as strings\n",
    "        # We'll average over multiple references\n",
    "        scores = []\n",
    "        for ref in reference:\n",
    "            score = meteor_score([ref.split()], hypothesis.split())\n",
    "            scores.append(score)\n",
    "        \n",
    "        return np.mean(scores)\n",
    "    \n",
    "    def compute_corpus_meteor(self, references: List[List[str]], hypotheses: List[str]) -> float:\n",
    "        \"\"\"Compute METEOR for entire corpus\"\"\"\n",
    "        scores = []\n",
    "        \n",
    "        for refs, hyp in zip(references, hypotheses):\n",
    "            score = self.compute_sentence_meteor(refs, hyp)\n",
    "            scores.append(score)\n",
    "        \n",
    "        return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6ee212",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ROUGEScorer:\n",
    "    \"\"\"\n",
    "    ROUGE-L: Measures longest common subsequence\n",
    "    Focus on recall rather than precision\n",
    "    \n",
    "    Range: 0-1 (higher is better)\n",
    "    Returns: Precision, Recall, F1-score\n",
    "    Good F1 score: > 0.4\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    \n",
    "    def compute_sentence_rouge(self, reference: List[str], hypothesis: str) -> Dict[str, float]:\n",
    "        \"\"\"Compute ROUGE-L for single sentence\"\"\"\n",
    "        scores_list = []\n",
    "        \n",
    "        for ref in reference:\n",
    "            score = self.scorer.score(ref, hypothesis)\n",
    "            scores_list.append({\n",
    "                'precision': score['rougeL'].precision,\n",
    "                'recall': score['rougeL'].recall,\n",
    "                'f1': score['rougeL'].fmeasure\n",
    "            })\n",
    "        \n",
    "        # Average over references\n",
    "        avg_scores = {\n",
    "            'precision': np.mean([s['precision'] for s in scores_list]),\n",
    "            'recall': np.mean([s['recall'] for s in scores_list]),\n",
    "            'f1': np.mean([s['f1'] for s in scores_list])\n",
    "        }\n",
    "        \n",
    "        return avg_scores\n",
    "    \n",
    "    def compute_corpus_rouge(self, references: List[List[str]], hypotheses: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"Compute ROUGE-L for entire corpus\"\"\"\n",
    "        all_precisions = []\n",
    "        all_recalls = []\n",
    "        all_f1s = []\n",
    "        \n",
    "        for refs, hyp in zip(references, hypotheses):\n",
    "            scores = self.compute_sentence_rouge(refs, hyp)\n",
    "            all_precisions.append(scores['precision'])\n",
    "            all_recalls.append(scores['recall'])\n",
    "            all_f1s.append(scores['f1'])\n",
    "        \n",
    "        return {\n",
    "            'ROUGE-L-P': np.mean(all_precisions),\n",
    "            'ROUGE-L-R': np.mean(all_recalls),\n",
    "            'ROUGE-L-F1': np.mean(all_f1s)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f209d1b0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CIDErScorer:\n",
    "    \"\"\"\n",
    "    CIDEr: Specialized metric for image captioning\n",
    "    Measures consensus between generated caption and human captions\n",
    "    Uses TF-IDF weighting\n",
    "    \n",
    "    Range: 0-10+ (higher is better)\n",
    "    Good score: > 0.8\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n=4, sigma=6.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n: max n-gram order\n",
    "            sigma: standard deviation for Gaussian penalty\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    def _compute_doc_freq(self, refs_ngrams):\n",
    "        \"\"\"Compute document frequency for IDF\"\"\"\n",
    "        doc_freq = defaultdict(int)\n",
    "        \n",
    "        for ngrams_dict in refs_ngrams:\n",
    "            for ngram in ngrams_dict.keys():\n",
    "                doc_freq[ngram] += 1\n",
    "        \n",
    "        return doc_freq\n",
    "    \n",
    "    def _get_ngrams(self, tokens, n):\n",
    "        \"\"\"Get n-grams from tokens\"\"\"\n",
    "        ngrams = defaultdict(int)\n",
    "        \n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            ngram = tuple(tokens[i:i+n])\n",
    "            ngrams[ngram] += 1\n",
    "        \n",
    "        return ngrams\n",
    "    \n",
    "    def compute_cider(self, references: List[List[str]], hypotheses: List[str]) -> float:\n",
    "        \"\"\"\n",
    "        Compute CIDEr score\n",
    "        \n",
    "        This is a simplified implementation. For exact CIDEr scores,\n",
    "        use the official pycocoevalcap package.\n",
    "        \"\"\"\n",
    "        scores = []\n",
    "        \n",
    "        # Get all n-grams\n",
    "        all_refs_ngrams = []\n",
    "        all_hyps_ngrams = []\n",
    "        \n",
    "        for refs, hyp in zip(references, hypotheses):\n",
    "            # Tokenize\n",
    "            ref_tokens_list = [ref.split() for ref in refs]\n",
    "            hyp_tokens = hyp.split()\n",
    "            \n",
    "            # Compute n-grams for references\n",
    "            refs_ngrams = []\n",
    "            for ref_tokens in ref_tokens_list:\n",
    "                ngrams_dict = {}\n",
    "                for n in range(1, self.n + 1):\n",
    "                    ngrams_dict.update(self._get_ngrams(ref_tokens, n))\n",
    "                refs_ngrams.append(ngrams_dict)\n",
    "            \n",
    "            # Compute n-grams for hypothesis\n",
    "            hyp_ngrams = {}\n",
    "            for n in range(1, self.n + 1):\n",
    "                hyp_ngrams.update(self._get_ngrams(hyp_tokens, n))\n",
    "            \n",
    "            all_refs_ngrams.append(refs_ngrams)\n",
    "            all_hyps_ngrams.append(hyp_ngrams)\n",
    "        \n",
    "        # Compute document frequencies\n",
    "        all_ngrams = []\n",
    "        for refs_ngrams in all_refs_ngrams:\n",
    "            for ngrams_dict in refs_ngrams:\n",
    "                all_ngrams.append(ngrams_dict)\n",
    "        \n",
    "        doc_freq = self._compute_doc_freq(all_ngrams)\n",
    "        num_docs = len(all_ngrams)\n",
    "        \n",
    "        # Compute CIDEr for each hypothesis\n",
    "        for refs_ngrams, hyp_ngrams in zip(all_refs_ngrams, all_hyps_ngrams):\n",
    "            # Compute TF-IDF vectors\n",
    "            vec_hyp = {}\n",
    "            vec_refs = []\n",
    "            \n",
    "            # Hypothesis vector\n",
    "            for ngram, count in hyp_ngrams.items():\n",
    "                tf = count / len(hyp_ngrams)\n",
    "                idf = np.log((num_docs + 1) / (doc_freq[ngram] + 1))\n",
    "                vec_hyp[ngram] = tf * idf\n",
    "            \n",
    "            # Reference vectors (average)\n",
    "            for ref_ngrams in refs_ngrams:\n",
    "                vec_ref = {}\n",
    "                for ngram, count in ref_ngrams.items():\n",
    "                    tf = count / len(ref_ngrams)\n",
    "                    idf = np.log((num_docs + 1) / (doc_freq[ngram] + 1))\n",
    "                    vec_ref[ngram] = tf * idf\n",
    "                vec_refs.append(vec_ref)\n",
    "            \n",
    "            # Compute cosine similarity\n",
    "            similarities = []\n",
    "            for vec_ref in vec_refs:\n",
    "                # Dot product\n",
    "                dot_product = sum(vec_hyp.get(k, 0) * v for k, v in vec_ref.items())\n",
    "                \n",
    "                # Norms\n",
    "                norm_hyp = np.sqrt(sum(v**2 for v in vec_hyp.values()))\n",
    "                norm_ref = np.sqrt(sum(v**2 for v in vec_ref.values()))\n",
    "                \n",
    "                if norm_hyp > 0 and norm_ref > 0:\n",
    "                    sim = dot_product / (norm_hyp * norm_ref)\n",
    "                else:\n",
    "                    sim = 0.0\n",
    "                \n",
    "                similarities.append(sim)\n",
    "            \n",
    "            # Average similarity\n",
    "            score = np.mean(similarities) * 10.0  # Scale to 0-10\n",
    "            scores.append(score)\n",
    "        \n",
    "        return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b518a5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CaptionEvaluator:\n",
    "    \"\"\"\n",
    "    Complete evaluator for image captioning\n",
    "    Computes all major metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.bleu_scorer = BLEUScorer()\n",
    "        self.meteor_scorer = METEORScorer()\n",
    "        self.rouge_scorer = ROUGEScorer()\n",
    "        self.cider_scorer = CIDErScorer()\n",
    "    \n",
    "    def evaluate(self, references: List[List[str]], hypotheses: List[str], \n",
    "                 verbose: bool = True) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate generated captions against references\n",
    "        \n",
    "        Args:\n",
    "            references: List of [list of reference captions] for each image\n",
    "            hypotheses: List of generated captions (one per image)\n",
    "            verbose: Print results\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with all metric scores\n",
    "        \"\"\"\n",
    "        if len(references) != len(hypotheses):\n",
    "            raise ValueError(\"Number of references and hypotheses must match\")\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # 1. BLEU scores\n",
    "        if verbose:\n",
    "            print(\"Computing BLEU scores...\")\n",
    "        bleu_scores = self.bleu_scorer.compute_corpus_bleu(references, hypotheses)\n",
    "        results.update(bleu_scores)\n",
    "        \n",
    "        # 2. METEOR score\n",
    "        if verbose:\n",
    "            print(\"Computing METEOR score...\")\n",
    "        meteor = self.meteor_scorer.compute_corpus_meteor(references, hypotheses)\n",
    "        results['METEOR'] = meteor\n",
    "        \n",
    "        # 3. ROUGE-L score\n",
    "        if verbose:\n",
    "            print(\"Computing ROUGE-L scores...\")\n",
    "        rouge_scores = self.rouge_scorer.compute_corpus_rouge(references, hypotheses)\n",
    "        results.update(rouge_scores)\n",
    "        \n",
    "        # 4. CIDEr score\n",
    "        if verbose:\n",
    "            print(\"Computing CIDEr score...\")\n",
    "        cider = self.cider_scorer.compute_cider(references, hypotheses)\n",
    "        results['CIDEr'] = cider\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"EVALUATION RESULTS\")\n",
    "            print(\"=\"*60)\n",
    "            print(f\"Number of samples: {len(hypotheses)}\")\n",
    "            print()\n",
    "            print(\"BLEU Scores:\")\n",
    "            print(f\"  BLEU-1:  {results['BLEU-1']:.4f}\")\n",
    "            print(f\"  BLEU-2:  {results['BLEU-2']:.4f}\")\n",
    "            print(f\"  BLEU-3:  {results['BLEU-3']:.4f}\")\n",
    "            print(f\"  BLEU-4:  {results['BLEU-4']:.4f}\")\n",
    "            print()\n",
    "            print(f\"METEOR:   {results['METEOR']:.4f}\")\n",
    "            print()\n",
    "            print(\"ROUGE-L Scores:\")\n",
    "            print(f\"  Precision: {results['ROUGE-L-P']:.4f}\")\n",
    "            print(f\"  Recall:    {results['ROUGE-L-R']:.4f}\")\n",
    "            print(f\"  F1-Score:  {results['ROUGE-L-F1']:.4f}\")\n",
    "            print()\n",
    "            print(f\"CIDEr:    {results['CIDEr']:.4f}\")\n",
    "            print(\"=\"*60)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def evaluate_single(self, references: List[str], hypothesis: str) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate single caption\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # BLEU\n",
    "        for n in [1, 2, 3, 4]:\n",
    "            score = self.bleu_scorer.compute_sentence_bleu(references, hypothesis, n)\n",
    "            results[f'BLEU-{n}'] = score\n",
    "        \n",
    "        # METEOR\n",
    "        results['METEOR'] = self.meteor_scorer.compute_sentence_meteor(references, hypothesis)\n",
    "        \n",
    "        # ROUGE-L\n",
    "        rouge = self.rouge_scorer.compute_sentence_rouge(references, hypothesis)\n",
    "        results['ROUGE-L-F1'] = rouge['f1']\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebe0592",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate_model(model, test_images: List[str], caption_dict: Dict[str, List[str]], \n",
    "                   img_folder: str, tokenizer, config, \n",
    "                   use_beam_search: bool = False) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate model on test set\n",
    "    \n",
    "    Args:\n",
    "        model: Trained captioning model\n",
    "        test_images: List of test image names\n",
    "        caption_dict: Dictionary mapping image names to reference captions\n",
    "        img_folder: Folder containing images\n",
    "        tokenizer: Fitted tokenizer\n",
    "        config: Configuration object\n",
    "        use_beam_search: Use beam search for generation\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with all evaluation metrics\n",
    "    \"\"\"\n",
    "    from image_captioning import generate_caption, generate_beam_search\n",
    "    \n",
    "    print(f\"Evaluating on {len(test_images)} test images...\")\n",
    "    \n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    \n",
    "    for i, img_name in enumerate(test_images):\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f\"Progress: {i+1}/{len(test_images)}\")\n",
    "        \n",
    "        img_path = os.path.join(img_folder, img_name)\n",
    "        \n",
    "        if not os.path.exists(img_path):\n",
    "            continue\n",
    "        \n",
    "        # Get reference captions\n",
    "        refs = caption_dict[img_name]\n",
    "        references.append(refs)\n",
    "        \n",
    "        # Generate caption\n",
    "        try:\n",
    "            if use_beam_search:\n",
    "                hyp = generate_beam_search(model, img_path, tokenizer, config)\n",
    "            else:\n",
    "                hyp = generate_caption(model, img_path, tokenizer, config)\n",
    "            hypotheses.append(hyp)\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating caption for {img_name}: {e}\")\n",
    "            hypotheses.append(\"\")  # Empty caption for failed generation\n",
    "    \n",
    "    # Evaluate\n",
    "    evaluator = CaptionEvaluator()\n",
    "    results = evaluator.evaluate(references, hypotheses, verbose=True)\n",
    "    \n",
    "    return results, references, hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfbdb90",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def analyze_predictions(references: List[List[str]], hypotheses: List[str], \n",
    "                        image_names: List[str] = None, n_samples: int = 10):\n",
    "    \"\"\"\n",
    "    Show qualitative examples of predictions\n",
    "    \n",
    "    Args:\n",
    "        references: Reference captions\n",
    "        hypotheses: Generated captions\n",
    "        image_names: Optional image names\n",
    "        n_samples: Number of samples to show\n",
    "    \"\"\"\n",
    "    evaluator = CaptionEvaluator()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"QUALITATIVE ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Select random samples\n",
    "    indices = np.random.choice(len(hypotheses), min(n_samples, len(hypotheses)), replace=False)\n",
    "    \n",
    "    for idx in indices:\n",
    "        refs = references[idx]\n",
    "        hyp = hypotheses[idx]\n",
    "        \n",
    "        # Compute metrics for this sample\n",
    "        scores = evaluator.evaluate_single(refs, hyp)\n",
    "        \n",
    "        print(f\"\\nSample {idx+1}\")\n",
    "        if image_names:\n",
    "            print(f\"Image: {image_names[idx]}\")\n",
    "        print(\"-\" * 80)\n",
    "        print(\"Reference Captions:\")\n",
    "        for i, ref in enumerate(refs, 1):\n",
    "            print(f\"  {i}. {ref}\")\n",
    "        print(f\"\\nGenerated Caption:\")\n",
    "        print(f\"  → {hyp}\")\n",
    "        print(f\"\\nScores:\")\n",
    "        print(f\"  BLEU-4: {scores['BLEU-4']:.4f}\")\n",
    "        print(f\"  METEOR: {scores['METEOR']:.4f}\")\n",
    "        print(f\"  ROUGE-L: {scores['ROUGE-L-F1']:.4f}\")\n",
    "        print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b9a221",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def save_evaluation_results(results: Dict[str, float], references: List[List[str]], \n",
    "                            hypotheses: List[str], image_names: List[str] = None,\n",
    "                            output_dir: str = \"evaluation_results\"):\n",
    "    \"\"\"Save evaluation results to files\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # 1. Save metrics\n",
    "    with open(os.path.join(output_dir, \"metrics.json\"), 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    # 2. Save predictions\n",
    "    predictions_data = []\n",
    "    for i, (refs, hyp) in enumerate(zip(references, hypotheses)):\n",
    "        entry = {\n",
    "            'id': i,\n",
    "            'references': refs,\n",
    "            'hypothesis': hyp\n",
    "        }\n",
    "        if image_names:\n",
    "            entry['image'] = image_names[i]\n",
    "        predictions_data.append(entry)\n",
    "    \n",
    "    with open(os.path.join(output_dir, \"predictions.json\"), 'w') as f:\n",
    "        json.dump(predictions_data, f, indent=2)\n",
    "    \n",
    "    # 3. Save as CSV for easy viewing\n",
    "    df = pd.DataFrame({\n",
    "        'image': image_names if image_names else range(len(hypotheses)),\n",
    "        'generated': hypotheses,\n",
    "        'reference_1': [refs[0] if len(refs) > 0 else \"\" for refs in references],\n",
    "        'reference_2': [refs[1] if len(refs) > 1 else \"\" for refs in references],\n",
    "    })\n",
    "    df.to_csv(os.path.join(output_dir, \"predictions.csv\"), index=False)\n",
    "    \n",
    "    print(f\"\\nResults saved to {output_dir}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f16aa2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def example_usage():\n",
    "    \"\"\"Example of how to use the evaluation functions\"\"\"\n",
    "    \n",
    "    # Example data\n",
    "    references = [\n",
    "        [\"a dog playing in the park\", \"a brown dog running on grass\"],\n",
    "        [\"a cat sitting on a chair\", \"an orange cat on furniture\"],\n",
    "        [\"a car on the street\", \"a red vehicle on the road\"]\n",
    "    ]\n",
    "    \n",
    "    hypotheses = [\n",
    "        \"a dog running in a park\",\n",
    "        \"a cat sitting on a chair\",\n",
    "        \"a red car on the street\"\n",
    "    ]\n",
    "    \n",
    "    # Evaluate\n",
    "    evaluator = CaptionEvaluator()\n",
    "    results = evaluator.evaluate(references, hypotheses, verbose=True)\n",
    "    \n",
    "    # Show qualitative analysis\n",
    "    analyze_predictions(references, hypotheses, n_samples=3)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945514a4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def main_evaluation(model_path: str, test_images: List[str], \n",
    "                    caption_dict: Dict[str, List[str]], img_folder: str,\n",
    "                    tokenizer_path: str, config, use_beam_search: bool = True):\n",
    "    \"\"\"\n",
    "    Complete evaluation pipeline\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to saved model\n",
    "        test_images: List of test image filenames\n",
    "        caption_dict: Dictionary of captions\n",
    "        img_folder: Folder containing images\n",
    "        tokenizer_path: Path to saved tokenizer\n",
    "        config: Configuration object\n",
    "        use_beam_search: Use beam search for generation\n",
    "    \"\"\"\n",
    "    import tensorflow as tf\n",
    "    import pickle\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    print(\"Loading model and tokenizer...\")\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    \n",
    "    with open(tokenizer_path, 'rb') as f:\n",
    "        tokenizer = pickle.load(f)\n",
    "    \n",
    "    # Evaluate\n",
    "    results, references, hypotheses = evaluate_model(\n",
    "        model, test_images, caption_dict, img_folder, \n",
    "        tokenizer, config, use_beam_search\n",
    "    )\n",
    "    \n",
    "    # Qualitative analysis\n",
    "    analyze_predictions(references, hypotheses, test_images, n_samples=10)\n",
    "    \n",
    "    # Save results\n",
    "    save_evaluation_results(results, references, hypotheses, test_images)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98b960b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Simple test\n",
    "    print(\"Testing evaluation metrics...\\n\")\n",
    "    example_usage()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
