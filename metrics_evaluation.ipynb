{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fdfd079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Tuple\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87914bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "                                              0.0/1.5 MB ? eta -:--:--\n",
      "                                              0.0/1.5 MB ? eta -:--:--\n",
      "                                              0.0/1.5 MB ? eta -:--:--\n",
      "     -                                        0.1/1.5 MB 1.9 MB/s eta 0:00:01\n",
      "     --                                       0.1/1.5 MB 1.3 MB/s eta 0:00:02\n",
      "     ---                                      0.1/1.5 MB 901.1 kB/s eta 0:00:02\n",
      "     ---                                      0.1/1.5 MB 774.0 kB/s eta 0:00:02\n",
      "     ----                                     0.2/1.5 MB 702.7 kB/s eta 0:00:02\n",
      "     ----                                     0.2/1.5 MB 702.7 kB/s eta 0:00:02\n",
      "     ----                                     0.2/1.5 MB 581.0 kB/s eta 0:00:03\n",
      "     ----                                     0.2/1.5 MB 581.0 kB/s eta 0:00:03\n",
      "     -----                                    0.2/1.5 MB 452.9 kB/s eta 0:00:03\n",
      "     -----                                    0.2/1.5 MB 452.9 kB/s eta 0:00:03\n",
      "     -----                                    0.2/1.5 MB 452.9 kB/s eta 0:00:03\n",
      "     -----                                    0.2/1.5 MB 377.1 kB/s eta 0:00:04\n",
      "     -----                                    0.2/1.5 MB 371.4 kB/s eta 0:00:04\n",
      "     -----                                    0.2/1.5 MB 371.4 kB/s eta 0:00:04\n",
      "     ------                                   0.2/1.5 MB 335.2 kB/s eta 0:00:04\n",
      "     ------                                   0.2/1.5 MB 335.2 kB/s eta 0:00:04\n",
      "     ------                                   0.3/1.5 MB 327.7 kB/s eta 0:00:04\n",
      "     ------                                   0.3/1.5 MB 327.7 kB/s eta 0:00:04\n",
      "     -------                                  0.3/1.5 MB 315.4 kB/s eta 0:00:04\n",
      "     -------                                  0.3/1.5 MB 310.3 kB/s eta 0:00:04\n",
      "     --------                                 0.3/1.5 MB 316.6 kB/s eta 0:00:04\n",
      "     --------                                 0.3/1.5 MB 327.7 kB/s eta 0:00:04\n",
      "     ---------                                0.4/1.5 MB 342.3 kB/s eta 0:00:04\n",
      "     ----------                               0.4/1.5 MB 346.3 kB/s eta 0:00:04\n",
      "     -----------                              0.4/1.5 MB 359.0 kB/s eta 0:00:04\n",
      "     ------------                             0.5/1.5 MB 387.9 kB/s eta 0:00:03\n",
      "     -------------                            0.5/1.5 MB 403.2 kB/s eta 0:00:03\n",
      "     --------------                           0.6/1.5 MB 423.3 kB/s eta 0:00:03\n",
      "     ---------------                          0.6/1.5 MB 446.9 kB/s eta 0:00:03\n",
      "     -----------------                        0.7/1.5 MB 469.2 kB/s eta 0:00:02\n",
      "     ------------------                       0.7/1.5 MB 489.7 kB/s eta 0:00:02\n",
      "     -------------------                      0.7/1.5 MB 500.0 kB/s eta 0:00:02\n",
      "     --------------------                     0.8/1.5 MB 499.8 kB/s eta 0:00:02\n",
      "     ---------------------                    0.8/1.5 MB 509.7 kB/s eta 0:00:02\n",
      "     ---------------------                    0.8/1.5 MB 512.3 kB/s eta 0:00:02\n",
      "     -----------------------                  0.9/1.5 MB 524.2 kB/s eta 0:00:02\n",
      "     ------------------------                 0.9/1.5 MB 533.8 kB/s eta 0:00:02\n",
      "     -------------------------                1.0/1.5 MB 548.9 kB/s eta 0:00:02\n",
      "     --------------------------               1.0/1.5 MB 558.4 kB/s eta 0:00:01\n",
      "     ----------------------------             1.1/1.5 MB 576.7 kB/s eta 0:00:01\n",
      "     -----------------------------            1.1/1.5 MB 589.8 kB/s eta 0:00:01\n",
      "     -------------------------------          1.2/1.5 MB 612.7 kB/s eta 0:00:01\n",
      "     ---------------------------------        1.2/1.5 MB 629.3 kB/s eta 0:00:01\n",
      "     -----------------------------------      1.3/1.5 MB 655.2 kB/s eta 0:00:01\n",
      "     ------------------------------------     1.4/1.5 MB 670.3 kB/s eta 0:00:01\n",
      "     --------------------------------------   1.5/1.5 MB 684.3 kB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.5/1.5 MB 696.9 kB/s eta 0:00:00\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.3.1-py3-none-any.whl (108 kB)\n",
      "                                              0.0/108.3 kB ? eta -:--:--\n",
      "     ----------------------                  61.4/108.3 kB 1.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- 108.3/108.3 kB 1.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: joblib in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (1.5.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2025.11.3-cp311-cp311-win_amd64.whl (277 kB)\n",
      "                                              0.0/277.7 kB ? eta -:--:--\n",
      "     --------                                61.4/277.7 kB 1.7 MB/s eta 0:00:01\n",
      "     -------------------                    143.4/277.7 kB 1.4 MB/s eta 0:00:01\n",
      "     -------------------------              184.3/277.7 kB 1.6 MB/s eta 0:00:01\n",
      "     -----------------------------------    256.0/277.7 kB 1.4 MB/s eta 0:00:01\n",
      "     -------------------------------------- 277.7/277.7 kB 1.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk) (0.4.6)\n",
      "Installing collected packages: regex, click, nltk\n",
      "Successfully installed click-8.3.1 nltk-3.9.2 regex-2025.11.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script nltk.exe is installed in 'c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d33b12b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: absl-py in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rouge_score) (2.3.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rouge_score) (3.9.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rouge_score) (2.3.4)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\asus\\appdata\\roaming\\python\\python311\\site-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: click in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk->rouge_score) (8.3.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk->rouge_score) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk->rouge_score) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk->rouge_score) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk->rouge_score) (0.4.6)\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py): started\n",
      "  Building wheel for rouge_score (setup.py): finished with status 'done'\n",
      "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24972 sha256=2602ea1f5f6aca5bfc2a61aee731dff356aa31d54ddf7c6f3bb0fdf0bed5d7d5\n",
      "  Stored in directory: c:\\users\\asus\\appdata\\local\\pip\\cache\\wheels\\1e\\19\\43\\8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: rouge_score\n",
      "Successfully installed rouge_score-0.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "017d0ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bee4162",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLEUScorer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.smoothing = SmoothingFunction()\n",
    "    \n",
    "    def compute_sentence_bleu(self, reference: List[str], hypothesis: str, n: int = 4) -> float:\n",
    "        # Tokenize\n",
    "        ref_tokens = [ref.split() for ref in reference]\n",
    "        hyp_tokens = hypothesis.split()\n",
    "        \n",
    "        # Set weights based on n\n",
    "        if n == 1:\n",
    "            weights = (1.0, 0, 0, 0)\n",
    "        elif n == 2:\n",
    "            weights = (0.5, 0.5, 0, 0)\n",
    "        elif n == 3:\n",
    "            weights = (0.33, 0.33, 0.33, 0)\n",
    "        else:  # n == 4\n",
    "            weights = (0.25, 0.25, 0.25, 0.25)\n",
    "        \n",
    "        # Compute BLEU\n",
    "        score = sentence_bleu(\n",
    "            ref_tokens, \n",
    "            hyp_tokens, \n",
    "            weights=weights,\n",
    "            smoothing_function=self.smoothing.method1\n",
    "        )\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def compute_corpus_bleu(self, references: List[List[str]], hypotheses: List[str]) -> Dict[str, float]:\n",
    "        # Tokenize all\n",
    "        refs_tokens = [[ref.split() for ref in refs] for refs in references]\n",
    "        hyps_tokens = [hyp.split() for hyp in hypotheses]\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # BLEU-1\n",
    "        results['BLEU-1'] = corpus_bleu(\n",
    "            refs_tokens, hyps_tokens, \n",
    "            weights=(1.0, 0, 0, 0),\n",
    "            smoothing_function=self.smoothing.method1\n",
    "        )\n",
    "        \n",
    "        # BLEU-2\n",
    "        results['BLEU-2'] = corpus_bleu(\n",
    "            refs_tokens, hyps_tokens, \n",
    "            weights=(0.5, 0.5, 0, 0),\n",
    "            smoothing_function=self.smoothing.method1\n",
    "        )\n",
    "        \n",
    "        # BLEU-3\n",
    "        results['BLEU-3'] = corpus_bleu(\n",
    "            refs_tokens, hyps_tokens, \n",
    "            weights=(0.33, 0.33, 0.33, 0),\n",
    "            smoothing_function=self.smoothing.method1\n",
    "        )\n",
    "        \n",
    "        # BLEU-4\n",
    "        results['BLEU-4'] = corpus_bleu(\n",
    "            refs_tokens, hyps_tokens, \n",
    "            weights=(0.25, 0.25, 0.25, 0.25),\n",
    "            smoothing_function=self.smoothing.method1\n",
    "        )\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8b84eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class METEORScorer:\n",
    "    \n",
    "    def compute_sentence_meteor(self, reference: List[str], hypothesis: str) -> float:\n",
    "        \"\"\"Compute METEOR for single sentence\"\"\"\n",
    "        # METEOR expects single reference and hypothesis as strings\n",
    "        # We'll average over multiple references\n",
    "        scores = []\n",
    "        for ref in reference:\n",
    "            score = meteor_score([ref.split()], hypothesis.split())\n",
    "            scores.append(score)\n",
    "        \n",
    "        return np.mean(scores)\n",
    "    \n",
    "    def compute_corpus_meteor(self, references: List[List[str]], hypotheses: List[str]) -> float:\n",
    "        \"\"\"Compute METEOR for entire corpus\"\"\"\n",
    "        scores = []\n",
    "        \n",
    "        for refs, hyp in zip(references, hypotheses):\n",
    "            score = self.compute_sentence_meteor(refs, hyp)\n",
    "            scores.append(score)\n",
    "        \n",
    "        return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03e9ad5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ROUGEScorer:\n",
    "    \"\"\"\n",
    "    ROUGE-L: Measures longest common subsequence\n",
    "    Focus on recall rather than precision\n",
    "    \n",
    "    Range: 0-1 (higher is better)\n",
    "    Returns: Precision, Recall, F1-score\n",
    "    Good F1 score: > 0.4\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    \n",
    "    def compute_sentence_rouge(self, reference: List[str], hypothesis: str) -> Dict[str, float]:\n",
    "        \"\"\"Compute ROUGE-L for single sentence\"\"\"\n",
    "        scores_list = []\n",
    "        \n",
    "        for ref in reference:\n",
    "            score = self.scorer.score(ref, hypothesis)\n",
    "            scores_list.append({\n",
    "                'precision': score['rougeL'].precision,\n",
    "                'recall': score['rougeL'].recall,\n",
    "                'f1': score['rougeL'].fmeasure\n",
    "            })\n",
    "        \n",
    "        # Average over references\n",
    "        avg_scores = {\n",
    "            'precision': np.mean([s['precision'] for s in scores_list]),\n",
    "            'recall': np.mean([s['recall'] for s in scores_list]),\n",
    "            'f1': np.mean([s['f1'] for s in scores_list])\n",
    "        }\n",
    "        \n",
    "        return avg_scores\n",
    "    \n",
    "    def compute_corpus_rouge(self, references: List[List[str]], hypotheses: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"Compute ROUGE-L for entire corpus\"\"\"\n",
    "        all_precisions = []\n",
    "        all_recalls = []\n",
    "        all_f1s = []\n",
    "        \n",
    "        for refs, hyp in zip(references, hypotheses):\n",
    "            scores = self.compute_sentence_rouge(refs, hyp)\n",
    "            all_precisions.append(scores['precision'])\n",
    "            all_recalls.append(scores['recall'])\n",
    "            all_f1s.append(scores['f1'])\n",
    "        \n",
    "        return {\n",
    "            'ROUGE-L-P': np.mean(all_precisions),\n",
    "            'ROUGE-L-R': np.mean(all_recalls),\n",
    "            'ROUGE-L-F1': np.mean(all_f1s)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81eb09ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIDErScorer:\n",
    "    \"\"\"\n",
    "    CIDEr: Specialized metric for image captioning\n",
    "    Measures consensus between generated caption and human captions\n",
    "    Uses TF-IDF weighting\n",
    "    \n",
    "    Range: 0-10+ (higher is better)\n",
    "    Good score: > 0.8\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n=4, sigma=6.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n: max n-gram order\n",
    "            sigma: standard deviation for Gaussian penalty\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    def _compute_doc_freq(self, refs_ngrams):\n",
    "        \"\"\"Compute document frequency for IDF\"\"\"\n",
    "        doc_freq = defaultdict(int)\n",
    "        \n",
    "        for ngrams_dict in refs_ngrams:\n",
    "            for ngram in ngrams_dict.keys():\n",
    "                doc_freq[ngram] += 1\n",
    "        \n",
    "        return doc_freq\n",
    "    \n",
    "    def _get_ngrams(self, tokens, n):\n",
    "        \"\"\"Get n-grams from tokens\"\"\"\n",
    "        ngrams = defaultdict(int)\n",
    "        \n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            ngram = tuple(tokens[i:i+n])\n",
    "            ngrams[ngram] += 1\n",
    "        \n",
    "        return ngrams\n",
    "    \n",
    "    def compute_cider(self, references: List[List[str]], hypotheses: List[str]) -> float:\n",
    "        \"\"\"\n",
    "        Compute CIDEr score\n",
    "        \n",
    "        This is a simplified implementation. For exact CIDEr scores,\n",
    "        use the official pycocoevalcap package.\n",
    "        \"\"\"\n",
    "        scores = []\n",
    "        \n",
    "        # Get all n-grams\n",
    "        all_refs_ngrams = []\n",
    "        all_hyps_ngrams = []\n",
    "        \n",
    "        for refs, hyp in zip(references, hypotheses):\n",
    "            # Tokenize\n",
    "            ref_tokens_list = [ref.split() for ref in refs]\n",
    "            hyp_tokens = hyp.split()\n",
    "            \n",
    "            # Compute n-grams for references\n",
    "            refs_ngrams = []\n",
    "            for ref_tokens in ref_tokens_list:\n",
    "                ngrams_dict = {}\n",
    "                for n in range(1, self.n + 1):\n",
    "                    ngrams_dict.update(self._get_ngrams(ref_tokens, n))\n",
    "                refs_ngrams.append(ngrams_dict)\n",
    "            \n",
    "            # Compute n-grams for hypothesis\n",
    "            hyp_ngrams = {}\n",
    "            for n in range(1, self.n + 1):\n",
    "                hyp_ngrams.update(self._get_ngrams(hyp_tokens, n))\n",
    "            \n",
    "            all_refs_ngrams.append(refs_ngrams)\n",
    "            all_hyps_ngrams.append(hyp_ngrams)\n",
    "        \n",
    "        # Compute document frequencies\n",
    "        all_ngrams = []\n",
    "        for refs_ngrams in all_refs_ngrams:\n",
    "            for ngrams_dict in refs_ngrams:\n",
    "                all_ngrams.append(ngrams_dict)\n",
    "        \n",
    "        doc_freq = self._compute_doc_freq(all_ngrams)\n",
    "        num_docs = len(all_ngrams)\n",
    "        \n",
    "        # Compute CIDEr for each hypothesis\n",
    "        for refs_ngrams, hyp_ngrams in zip(all_refs_ngrams, all_hyps_ngrams):\n",
    "            # Compute TF-IDF vectors\n",
    "            vec_hyp = {}\n",
    "            vec_refs = []\n",
    "            \n",
    "            # Hypothesis vector\n",
    "            for ngram, count in hyp_ngrams.items():\n",
    "                tf = count / len(hyp_ngrams)\n",
    "                idf = np.log((num_docs + 1) / (doc_freq[ngram] + 1))\n",
    "                vec_hyp[ngram] = tf * idf\n",
    "            \n",
    "            # Reference vectors (average)\n",
    "            for ref_ngrams in refs_ngrams:\n",
    "                vec_ref = {}\n",
    "                for ngram, count in ref_ngrams.items():\n",
    "                    tf = count / len(ref_ngrams)\n",
    "                    idf = np.log((num_docs + 1) / (doc_freq[ngram] + 1))\n",
    "                    vec_ref[ngram] = tf * idf\n",
    "                vec_refs.append(vec_ref)\n",
    "            \n",
    "            # Compute cosine similarity\n",
    "            similarities = []\n",
    "            for vec_ref in vec_refs:\n",
    "                # Dot product\n",
    "                dot_product = sum(vec_hyp.get(k, 0) * v for k, v in vec_ref.items())\n",
    "                \n",
    "                # Norms\n",
    "                norm_hyp = np.sqrt(sum(v**2 for v in vec_hyp.values()))\n",
    "                norm_ref = np.sqrt(sum(v**2 for v in vec_ref.values()))\n",
    "                \n",
    "                if norm_hyp > 0 and norm_ref > 0:\n",
    "                    sim = dot_product / (norm_hyp * norm_ref)\n",
    "                else:\n",
    "                    sim = 0.0\n",
    "                \n",
    "                similarities.append(sim)\n",
    "            \n",
    "            # Average similarity\n",
    "            score = np.mean(similarities) * 10.0  # Scale to 0-10\n",
    "            scores.append(score)\n",
    "        \n",
    "        return np.mean(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de457223",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionEvaluator:\n",
    "    \"\"\"\n",
    "    Complete evaluator for image captioning\n",
    "    Computes all major metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.bleu_scorer = BLEUScorer()\n",
    "        self.meteor_scorer = METEORScorer()\n",
    "        self.rouge_scorer = ROUGEScorer()\n",
    "        self.cider_scorer = CIDErScorer()\n",
    "    \n",
    "    def evaluate(self, references: List[List[str]], hypotheses: List[str], \n",
    "                 verbose: bool = True) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate generated captions against references\n",
    "        \n",
    "        Args:\n",
    "            references: List of [list of reference captions] for each image\n",
    "            hypotheses: List of generated captions (one per image)\n",
    "            verbose: Print results\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with all metric scores\n",
    "        \"\"\"\n",
    "        if len(references) != len(hypotheses):\n",
    "            raise ValueError(\"Number of references and hypotheses must match\")\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # 1. BLEU scores\n",
    "        if verbose:\n",
    "            print(\"Computing BLEU scores...\")\n",
    "        bleu_scores = self.bleu_scorer.compute_corpus_bleu(references, hypotheses)\n",
    "        results.update(bleu_scores)\n",
    "        \n",
    "        # 2. METEOR score\n",
    "        if verbose:\n",
    "            print(\"Computing METEOR score...\")\n",
    "        meteor = self.meteor_scorer.compute_corpus_meteor(references, hypotheses)\n",
    "        results['METEOR'] = meteor\n",
    "        \n",
    "        # 3. ROUGE-L score\n",
    "        if verbose:\n",
    "            print(\"Computing ROUGE-L scores...\")\n",
    "        rouge_scores = self.rouge_scorer.compute_corpus_rouge(references, hypotheses)\n",
    "        results.update(rouge_scores)\n",
    "        \n",
    "        # 4. CIDEr score\n",
    "        if verbose:\n",
    "            print(\"Computing CIDEr score...\")\n",
    "        cider = self.cider_scorer.compute_cider(references, hypotheses)\n",
    "        results['CIDEr'] = cider\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"EVALUATION RESULTS\")\n",
    "            print(\"=\"*60)\n",
    "            print(f\"Number of samples: {len(hypotheses)}\")\n",
    "            print()\n",
    "            print(\"BLEU Scores:\")\n",
    "            print(f\"  BLEU-1:  {results['BLEU-1']:.4f}\")\n",
    "            print(f\"  BLEU-2:  {results['BLEU-2']:.4f}\")\n",
    "            print(f\"  BLEU-3:  {results['BLEU-3']:.4f}\")\n",
    "            print(f\"  BLEU-4:  {results['BLEU-4']:.4f}\")\n",
    "            print()\n",
    "            print(f\"METEOR:   {results['METEOR']:.4f}\")\n",
    "            print()\n",
    "            print(\"ROUGE-L Scores:\")\n",
    "            print(f\"  Precision: {results['ROUGE-L-P']:.4f}\")\n",
    "            print(f\"  Recall:    {results['ROUGE-L-R']:.4f}\")\n",
    "            print(f\"  F1-Score:  {results['ROUGE-L-F1']:.4f}\")\n",
    "            print()\n",
    "            print(f\"CIDEr:    {results['CIDEr']:.4f}\")\n",
    "            print(\"=\"*60)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def evaluate_single(self, references: List[str], hypothesis: str) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate single caption\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # BLEU\n",
    "        for n in [1, 2, 3, 4]:\n",
    "            score = self.bleu_scorer.compute_sentence_bleu(references, hypothesis, n)\n",
    "            results[f'BLEU-{n}'] = score\n",
    "        \n",
    "        # METEOR\n",
    "        results['METEOR'] = self.meteor_scorer.compute_sentence_meteor(references, hypothesis)\n",
    "        \n",
    "        # ROUGE-L\n",
    "        rouge = self.rouge_scorer.compute_sentence_rouge(references, hypothesis)\n",
    "        results['ROUGE-L-F1'] = rouge['f1']\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79b8dd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_images: List[str], caption_dict: Dict[str, List[str]], \n",
    "                   img_folder: str, tokenizer, config, \n",
    "                   use_beam_search: bool = False) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate model on test set\n",
    "    \n",
    "    Args:\n",
    "        model: Trained captioning model\n",
    "        test_images: List of test image names\n",
    "        caption_dict: Dictionary mapping image names to reference captions\n",
    "        img_folder: Folder containing images\n",
    "        tokenizer: Fitted tokenizer\n",
    "        config: Configuration object\n",
    "        use_beam_search: Use beam search for generation\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with all evaluation metrics\n",
    "    \"\"\"\n",
    "    from image_captioning import generate_caption, generate_beam_search\n",
    "    \n",
    "    print(f\"Evaluating on {len(test_images)} test images...\")\n",
    "    \n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    \n",
    "    for i, img_name in enumerate(test_images):\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f\"Progress: {i+1}/{len(test_images)}\")\n",
    "        \n",
    "        img_path = os.path.join(img_folder, img_name)\n",
    "        \n",
    "        if not os.path.exists(img_path):\n",
    "            continue\n",
    "        \n",
    "        # Get reference captions\n",
    "        refs = caption_dict[img_name]\n",
    "        references.append(refs)\n",
    "        \n",
    "        # Generate caption\n",
    "        try:\n",
    "            if use_beam_search:\n",
    "                hyp = generate_beam_search(model, img_path, tokenizer, config)\n",
    "            else:\n",
    "                hyp = generate_caption(model, img_path, tokenizer, config)\n",
    "            hypotheses.append(hyp)\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating caption for {img_name}: {e}\")\n",
    "            hypotheses.append(\"\")  # Empty caption for failed generation\n",
    "    \n",
    "    # Evaluate\n",
    "    evaluator = CaptionEvaluator()\n",
    "    results = evaluator.evaluate(references, hypotheses, verbose=True)\n",
    "    \n",
    "    return results, references, hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a54d3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_predictions(references: List[List[str]], hypotheses: List[str], \n",
    "                        image_names: List[str] = None, n_samples: int = 10):\n",
    "    \"\"\"\n",
    "    Show qualitative examples of predictions\n",
    "    \n",
    "    Args:\n",
    "        references: Reference captions\n",
    "        hypotheses: Generated captions\n",
    "        image_names: Optional image names\n",
    "        n_samples: Number of samples to show\n",
    "    \"\"\"\n",
    "    evaluator = CaptionEvaluator()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"QUALITATIVE ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Select random samples\n",
    "    indices = np.random.choice(len(hypotheses), min(n_samples, len(hypotheses)), replace=False)\n",
    "    \n",
    "    for idx in indices:\n",
    "        refs = references[idx]\n",
    "        hyp = hypotheses[idx]\n",
    "        \n",
    "        # Compute metrics for this sample\n",
    "        scores = evaluator.evaluate_single(refs, hyp)\n",
    "        \n",
    "        print(f\"\\nSample {idx+1}\")\n",
    "        if image_names:\n",
    "            print(f\"Image: {image_names[idx]}\")\n",
    "        print(\"-\" * 80)\n",
    "        print(\"Reference Captions:\")\n",
    "        for i, ref in enumerate(refs, 1):\n",
    "            print(f\"  {i}. {ref}\")\n",
    "        print(f\"\\nGenerated Caption:\")\n",
    "        print(f\"  â†’ {hyp}\")\n",
    "        print(f\"\\nScores:\")\n",
    "        print(f\"  BLEU-4: {scores['BLEU-4']:.4f}\")\n",
    "        print(f\"  METEOR: {scores['METEOR']:.4f}\")\n",
    "        print(f\"  ROUGE-L: {scores['ROUGE-L-F1']:.4f}\")\n",
    "        print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "827d9293",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_evaluation_results(results: Dict[str, float], references: List[List[str]], \n",
    "                            hypotheses: List[str], image_names: List[str] = None,\n",
    "                            output_dir: str = \"evaluation_results\"):\n",
    "    \"\"\"Save evaluation results to files\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # 1. Save metrics\n",
    "    with open(os.path.join(output_dir, \"metrics.json\"), 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    # 2. Save predictions\n",
    "    predictions_data = []\n",
    "    for i, (refs, hyp) in enumerate(zip(references, hypotheses)):\n",
    "        entry = {\n",
    "            'id': i,\n",
    "            'references': refs,\n",
    "            'hypothesis': hyp\n",
    "        }\n",
    "        if image_names:\n",
    "            entry['image'] = image_names[i]\n",
    "        predictions_data.append(entry)\n",
    "    \n",
    "    with open(os.path.join(output_dir, \"predictions.json\"), 'w') as f:\n",
    "        json.dump(predictions_data, f, indent=2)\n",
    "    \n",
    "    # 3. Save as CSV for easy viewing\n",
    "    df = pd.DataFrame({\n",
    "        'image': image_names if image_names else range(len(hypotheses)),\n",
    "        'generated': hypotheses,\n",
    "        'reference_1': [refs[0] if len(refs) > 0 else \"\" for refs in references],\n",
    "        'reference_2': [refs[1] if len(refs) > 1 else \"\" for refs in references],\n",
    "    })\n",
    "    df.to_csv(os.path.join(output_dir, \"predictions.csv\"), index=False)\n",
    "    \n",
    "    print(f\"\\nResults saved to {output_dir}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34d5448c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_usage():\n",
    "    \"\"\"Example of how to use the evaluation functions\"\"\"\n",
    "    \n",
    "    # Example data\n",
    "    references = [\n",
    "        [\"a dog playing in the park\", \"a brown dog running on grass\"],\n",
    "        [\"a cat sitting on a chair\", \"an orange cat on furniture\"],\n",
    "        [\"a car on the street\", \"a red vehicle on the road\"]\n",
    "    ]\n",
    "    \n",
    "    hypotheses = [\n",
    "        \"a dog running in a park\",\n",
    "        \"a cat sitting on a chair\",\n",
    "        \"a red car on the street\"\n",
    "    ]\n",
    "    \n",
    "    # Evaluate\n",
    "    evaluator = CaptionEvaluator()\n",
    "    results = evaluator.evaluate(references, hypotheses, verbose=True)\n",
    "    \n",
    "    # Show qualitative analysis\n",
    "    analyze_predictions(references, hypotheses, n_samples=3)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62cf4558",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_evaluation(model_path: str, test_images: List[str], \n",
    "                    caption_dict: Dict[str, List[str]], img_folder: str,\n",
    "                    tokenizer_path: str, config, use_beam_search: bool = True):\n",
    "    \"\"\"\n",
    "    Complete evaluation pipeline\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to saved model\n",
    "        test_images: List of test image filenames\n",
    "        caption_dict: Dictionary of captions\n",
    "        img_folder: Folder containing images\n",
    "        tokenizer_path: Path to saved tokenizer\n",
    "        config: Configuration object\n",
    "        use_beam_search: Use beam search for generation\n",
    "    \"\"\"\n",
    "    import tensorflow as tf\n",
    "    import pickle\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    print(\"Loading model and tokenizer...\")\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    \n",
    "    with open(tokenizer_path, 'rb') as f:\n",
    "        tokenizer = pickle.load(f)\n",
    "    \n",
    "    # Evaluate\n",
    "    results, references, hypotheses = evaluate_model(\n",
    "        model, test_images, caption_dict, img_folder, \n",
    "        tokenizer, config, use_beam_search\n",
    "    )\n",
    "    \n",
    "    # Qualitative analysis\n",
    "    analyze_predictions(references, hypotheses, test_images, n_samples=10)\n",
    "    \n",
    "    # Save results\n",
    "    save_evaluation_results(results, references, hypotheses, test_images)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68e3aab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing evaluation metrics...\n",
      "\n",
      "Computing BLEU scores...\n",
      "Computing METEOR score...\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ASUS/nltk_data'\n    - 'c:\\\\Users\\\\ASUS\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\nltk_data'\n    - 'c:\\\\Users\\\\ASUS\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\ASUS\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ASUS\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[39m, in \u001b[36mLazyCorpusLoader.__load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     root = \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ASUS/nltk_data'\n    - 'c:\\\\Users\\\\ASUS\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\nltk_data'\n    - 'c:\\\\Users\\\\ASUS\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\ASUS\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ASUS\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m# Simple test\u001b[39;00m\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTesting evaluation metrics...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[43mexample_usage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mexample_usage\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[32m     18\u001b[39m evaluator = CaptionEvaluator()\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m results = \u001b[43mevaluator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreferences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhypotheses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Show qualitative analysis\u001b[39;00m\n\u001b[32m     22\u001b[39m analyze_predictions(references, hypotheses, n_samples=\u001b[32m3\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mCaptionEvaluator.evaluate\u001b[39m\u001b[34m(self, references, hypotheses, verbose)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[32m     39\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mComputing METEOR score...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m meteor = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmeteor_scorer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute_corpus_meteor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreferences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhypotheses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m results[\u001b[33m'\u001b[39m\u001b[33mMETEOR\u001b[39m\u001b[33m'\u001b[39m] = meteor\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# 3. ROUGE-L score\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mMETEORScorer.compute_corpus_meteor\u001b[39m\u001b[34m(self, references, hypotheses)\u001b[39m\n\u001b[32m     16\u001b[39m scores = []\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m refs, hyp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(references, hypotheses):\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     score = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_sentence_meteor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m     scores.append(score)\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.mean(scores)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mMETEORScorer.compute_sentence_meteor\u001b[39m\u001b[34m(self, reference, hypothesis)\u001b[39m\n\u001b[32m      7\u001b[39m scores = []\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ref \u001b[38;5;129;01min\u001b[39;00m reference:\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     score = \u001b[43mmeteor_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mref\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhypothesis\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     scores.append(score)\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.mean(scores)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\translate\\meteor_score.py:397\u001b[39m, in \u001b[36mmeteor_score\u001b[39m\u001b[34m(references, hypothesis, preprocess, stemmer, wordnet, alpha, beta, gamma)\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmeteor_score\u001b[39m(\n\u001b[32m    348\u001b[39m     references: Iterable[Iterable[\u001b[38;5;28mstr\u001b[39m]],\n\u001b[32m    349\u001b[39m     hypothesis: Iterable[\u001b[38;5;28mstr\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    355\u001b[39m     gamma: \u001b[38;5;28mfloat\u001b[39m = \u001b[32m0.5\u001b[39m,\n\u001b[32m    356\u001b[39m ) -> \u001b[38;5;28mfloat\u001b[39m:\n\u001b[32m    357\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    358\u001b[39m \u001b[33;03m    Calculates METEOR score for hypothesis with multiple references as\u001b[39;00m\n\u001b[32m    359\u001b[39m \u001b[33;03m    described in \"Meteor: An Automatic Metric for MT Evaluation with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    395\u001b[39m \u001b[33;03m    :return: The sentence-level METEOR score.\u001b[39;00m\n\u001b[32m    396\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m397\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m        \u001b[49m\u001b[43msingle_meteor_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhypothesis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstemmer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstemmer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m            \u001b[49m\u001b[43mwordnet\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwordnet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m            \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreference\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreferences\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\translate\\meteor_score.py:398\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmeteor_score\u001b[39m(\n\u001b[32m    348\u001b[39m     references: Iterable[Iterable[\u001b[38;5;28mstr\u001b[39m]],\n\u001b[32m    349\u001b[39m     hypothesis: Iterable[\u001b[38;5;28mstr\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    355\u001b[39m     gamma: \u001b[38;5;28mfloat\u001b[39m = \u001b[32m0.5\u001b[39m,\n\u001b[32m    356\u001b[39m ) -> \u001b[38;5;28mfloat\u001b[39m:\n\u001b[32m    357\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    358\u001b[39m \u001b[33;03m    Calculates METEOR score for hypothesis with multiple references as\u001b[39;00m\n\u001b[32m    359\u001b[39m \u001b[33;03m    described in \"Meteor: An Automatic Metric for MT Evaluation with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    395\u001b[39m \u001b[33;03m    :return: The sentence-level METEOR score.\u001b[39;00m\n\u001b[32m    396\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    397\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmax\u001b[39m(\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m         \u001b[43msingle_meteor_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhypothesis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstemmer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstemmer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m            \u001b[49m\u001b[43mwordnet\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwordnet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m            \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    408\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m reference \u001b[38;5;129;01min\u001b[39;00m references\n\u001b[32m    409\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\translate\\meteor_score.py:331\u001b[39m, in \u001b[36msingle_meteor_score\u001b[39m\u001b[34m(reference, hypothesis, preprocess, stemmer, wordnet, alpha, beta, gamma)\u001b[39m\n\u001b[32m    329\u001b[39m translation_length = \u001b[38;5;28mlen\u001b[39m(enum_hypothesis)\n\u001b[32m    330\u001b[39m reference_length = \u001b[38;5;28mlen\u001b[39m(enum_reference)\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m matches, _, _ = \u001b[43m_enum_align_words\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m    \u001b[49m\u001b[43menum_hypothesis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menum_reference\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstemmer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstemmer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwordnet\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwordnet\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    334\u001b[39m matches_count = \u001b[38;5;28mlen\u001b[39m(matches)\n\u001b[32m    335\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\translate\\meteor_score.py:223\u001b[39m, in \u001b[36m_enum_align_words\u001b[39m\u001b[34m(enum_hypothesis_list, enum_reference_list, stemmer, wordnet)\u001b[39m\n\u001b[32m    215\u001b[39m exact_matches, enum_hypothesis_list, enum_reference_list = _match_enums(\n\u001b[32m    216\u001b[39m     enum_hypothesis_list, enum_reference_list\n\u001b[32m    217\u001b[39m )\n\u001b[32m    219\u001b[39m stem_matches, enum_hypothesis_list, enum_reference_list = _enum_stem_match(\n\u001b[32m    220\u001b[39m     enum_hypothesis_list, enum_reference_list, stemmer=stemmer\n\u001b[32m    221\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m wns_matches, enum_hypothesis_list, enum_reference_list = \u001b[43m_enum_wordnetsyn_match\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m    \u001b[49m\u001b[43menum_hypothesis_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menum_reference_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwordnet\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwordnet\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m    228\u001b[39m     \u001b[38;5;28msorted\u001b[39m(\n\u001b[32m    229\u001b[39m         exact_matches + stem_matches + wns_matches, key=\u001b[38;5;28;01mlambda\u001b[39;00m wordpair: wordpair[\u001b[32m0\u001b[39m]\n\u001b[32m   (...)\u001b[39m\u001b[32m    232\u001b[39m     enum_reference_list,\n\u001b[32m    233\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\translate\\meteor_score.py:161\u001b[39m, in \u001b[36m_enum_wordnetsyn_match\u001b[39m\u001b[34m(enum_hypothesis_list, enum_reference_list, wordnet)\u001b[39m\n\u001b[32m    152\u001b[39m word_match = []\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(enum_hypothesis_list))[::-\u001b[32m1\u001b[39m]:\n\u001b[32m    154\u001b[39m     hypothesis_syns = \u001b[38;5;28mset\u001b[39m(\n\u001b[32m    155\u001b[39m         chain.from_iterable(\n\u001b[32m    156\u001b[39m             (\n\u001b[32m    157\u001b[39m                 lemma.name()\n\u001b[32m    158\u001b[39m                 \u001b[38;5;28;01mfor\u001b[39;00m lemma \u001b[38;5;129;01min\u001b[39;00m synset.lemmas()\n\u001b[32m    159\u001b[39m                 \u001b[38;5;28;01mif\u001b[39;00m lemma.name().find(\u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m) < \u001b[32m0\u001b[39m\n\u001b[32m    160\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m synset \u001b[38;5;129;01min\u001b[39;00m \u001b[43mwordnet\u001b[49m\u001b[43m.\u001b[49m\u001b[43msynsets\u001b[49m(enum_hypothesis_list[i][\u001b[32m1\u001b[39m])\n\u001b[32m    162\u001b[39m         )\n\u001b[32m    163\u001b[39m     ).union({enum_hypothesis_list[i][\u001b[32m1\u001b[39m]})\n\u001b[32m    164\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(enum_reference_list))[::-\u001b[32m1\u001b[39m]:\n\u001b[32m    165\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m enum_reference_list[j][\u001b[32m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m hypothesis_syns:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\corpus\\util.py:120\u001b[39m, in \u001b[36mLazyCorpusLoader.__getattr__\u001b[39m\u001b[34m(self, attr)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attr == \u001b[33m\"\u001b[39m\u001b[33m__bases__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mLazyCorpusLoader object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[33m__bases__\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[32m    122\u001b[39m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[39m, in \u001b[36mLazyCorpusLoader.__load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     84\u001b[39m             root = nltk.data.find(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.subdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     85\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[32m     89\u001b[39m corpus = \u001b[38;5;28mself\u001b[39m.__reader_cls(root, *\u001b[38;5;28mself\u001b[39m.__args, **\u001b[38;5;28mself\u001b[39m.__kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[39m, in \u001b[36mLazyCorpusLoader.__load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m         root = \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     83\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ASUS/nltk_data'\n    - 'c:\\\\Users\\\\ASUS\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\nltk_data'\n    - 'c:\\\\Users\\\\ASUS\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\ASUS\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ASUS\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Simple test\n",
    "    print(\"Testing evaluation metrics...\\n\")\n",
    "    example_usage()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
